{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzeW4BhXYYgX"
      },
      "source": [
        "# Download SoccerNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncpMGt3OVQTW"
      },
      "outputs": [],
      "source": [
        "# Installazione librerie necessarie\n",
        "!pip install SoccerNet\n",
        "\n",
        "import os\n",
        "from SoccerNet.Downloader import SoccerNetDownloader\n",
        "\n",
        "# Impostiamo un percorso universale (una cartella 'dataset' locale al notebook)\n",
        "base_dir = './dataset_soccernet'\n",
        "\n",
        "if not os.path.exists(base_dir):\n",
        "    os.makedirs(base_dir)\n",
        "    print(f\"Creata cartella: {base_dir}\")\n",
        "else:\n",
        "    print(f\"Cartella gi√† esistente: {base_dir}\")\n",
        "\n",
        "# Inizializza il downloader\n",
        "mySoccerNetDownloader = SoccerNetDownloader(LocalDirectory=base_dir)\n",
        "\n",
        "# --- 1. Scaricare i dati di TRACKING ---\n",
        "print(\"Inizio download Tracking Data...\")\n",
        "mySoccerNetDownloader.downloadDataTask(task=\"tracking\", split=[\"challenge\"])\n",
        "\n",
        "# --- 2. Scaricare i dati di RE-IDENTIFICATION ---\n",
        "print(\"Inizio download Re-ID Data...\")\n",
        "mySoccerNetDownloader.downloadDataTask(task=\"reid\", split=[\"train\", \"valid\", \"test\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GCskZsZhLnF"
      },
      "source": [
        "\n",
        "# Conversione training per YOLO\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 1. SETUP ENVIRONMENT & LIBRARIES\n",
        "# ==========================================\n",
        "import os\n",
        "import shutil\n",
        "import configparser\n",
        "import glob\n",
        "from tqdm.notebook import tqdm\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Installa Ultralytics (YOLO) se non presente\n",
        "try:\n",
        "    import ultralytics\n",
        "    print(\"‚úÖ Ultralytics gi√† installato.\")\n",
        "except ImportError:\n",
        "    print(\"‚¨áÔ∏è Installazione Ultralytics in corso...\")\n",
        "    !pip install ultralytics\n",
        "    clear_output()\n",
        "    print(\"‚úÖ Installazione completata.\")\n",
        "\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üî• GPU Attiva: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è ATTENZIONE: Stai usando la CPU! Attiva un acceleratore hardware per il training.\")\n",
        "\n",
        "# ==========================================\n",
        "# 2. CONFIGURAZIONE PERCORSI (Relative Paths)\n",
        "# ==========================================\n",
        "# Istruzione per chi clona la repo: Assicurati di avere il file zip del dataset\n",
        "# nella stessa cartella di questo notebook, oppure modifica 'LOCAL_ZIP_PATH'.\n",
        "LOCAL_ZIP_PATH = './test.zip'\n",
        "RAW_DATA_DIR = './dataset_raw'\n",
        "YOLO_DATA_DIR = './dataset_yolo'\n",
        "\n",
        "# ==========================================\n",
        "# 3. ESTRAZIONE DATASET\n",
        "# ==========================================\n",
        "if not os.path.exists(RAW_DATA_DIR):\n",
        "    if os.path.exists(LOCAL_ZIP_PATH):\n",
        "        print(f\"‚è≥ Estrazione di {LOCAL_ZIP_PATH} in corso...\")\n",
        "        shutil.unpack_archive(LOCAL_ZIP_PATH, RAW_DATA_DIR)\n",
        "        print(f\"‚úÖ Dataset estratto in: {RAW_DATA_DIR}\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è ATTENZIONE: File {LOCAL_ZIP_PATH} non trovato. \"\n",
        "              f\"Se non hai i dati grezzi, assicurati di scaricarli prima di procedere.\")\n",
        "else:\n",
        "    print(f\"‚úÖ Dataset gi√† presente in: {RAW_DATA_DIR}. Salto l'estrazione.\")\n",
        "\n",
        "# ==========================================\n",
        "# 4. PREPROCESSING & CUSTOM FILTERING\n",
        "# ==========================================\n",
        "# Questa sezione converte le annotazioni nel formato YOLO\n",
        "# e implementa un filtro personalizzato per escludere la palla (ball tracklets).\n",
        "\n",
        "def get_ball_ids(ini_path):\n",
        "    \"\"\"Legge il file INI e restituisce gli ID corrispondenti alla palla.\"\"\"\n",
        "    ball_ids = set()\n",
        "    if not os.path.exists(ini_path): return ball_ids\n",
        "\n",
        "    try:\n",
        "        with open(ini_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if line.startswith('trackletID_') and 'ball' in line.lower():\n",
        "                    try:\n",
        "                        key_part = line.split('=')[0].strip()\n",
        "                        track_id = int(key_part.split('_')[1])\n",
        "                        ball_ids.add(track_id)\n",
        "                    except (IndexError, ValueError):\n",
        "                        continue\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Errore lettura INI {ini_path}: {e}\")\n",
        "    return ball_ids\n",
        "\n",
        "def convert_soccernet_clean(source_dir, dest_dir):\n",
        "    \"\"\"Converte le annotazioni SoccerNet in formato YOLO filtrando la palla.\"\"\"\n",
        "    if os.path.exists(dest_dir):\n",
        "        print(f\"üßπ Pulizia cartella destinazione ({dest_dir})...\")\n",
        "        shutil.rmtree(dest_dir)\n",
        "\n",
        "    img_dest_path = os.path.join(dest_dir, 'images', 'train')\n",
        "    lbl_dest_path = os.path.join(dest_dir, 'labels', 'train')\n",
        "    os.makedirs(img_dest_path, exist_ok=True)\n",
        "    os.makedirs(lbl_dest_path, exist_ok=True)\n",
        "\n",
        "    print(\"üîç Ricerca file gt.txt in corso...\")\n",
        "    gt_files = glob.glob(os.path.join(source_dir, '**', 'gt.txt'), recursive=True)\n",
        "\n",
        "    if not gt_files:\n",
        "        print(\"‚ùå ERRORE: Nessun file gt.txt trovato nella cartella sorgente!\")\n",
        "        return\n",
        "\n",
        "    print(f\"üìÇ Trovate {len(gt_files)} sequenze. Inizio conversione...\")\n",
        "\n",
        "    total_frames = 0\n",
        "    total_boxes = 0\n",
        "    removed_balls = 0\n",
        "\n",
        "    for gt_path in gt_files:\n",
        "        gt_folder = os.path.dirname(gt_path)\n",
        "        seq_folder = os.path.dirname(gt_folder)\n",
        "        seq_name = os.path.basename(seq_folder)\n",
        "        img_folder = os.path.join(seq_folder, 'img1')\n",
        "\n",
        "        ini_file = os.path.join(seq_folder, 'gameinfo.ini')\n",
        "        if not os.path.exists(ini_file):\n",
        "            ini_file = os.path.join(seq_folder, 'seqinfo.ini')\n",
        "\n",
        "        if not os.path.exists(img_folder): continue\n",
        "\n",
        "        images = []\n",
        "        for ext in ('*.jpg', '*.jpeg', '*.png', '*.JPG'):\n",
        "            images.extend(glob.glob(os.path.join(img_folder, ext)))\n",
        "\n",
        "        if not images: continue\n",
        "\n",
        "        ids_to_ignore = get_ball_ids(ini_file)\n",
        "\n",
        "        W, H = 1920, 1080\n",
        "        if os.path.exists(ini_file):\n",
        "            cfg = configparser.ConfigParser()\n",
        "            try:\n",
        "                cfg.read(ini_file)\n",
        "                if 'Sequence' in cfg:\n",
        "                    W = int(cfg['Sequence'].get('imWidth', 1920))\n",
        "                    H = int(cfg['Sequence'].get('imHeight', 1080))\n",
        "            except: pass\n",
        "\n",
        "        anns = {}\n",
        "        with open(gt_path, 'r') as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split(',')\n",
        "                if len(parts) < 6: continue\n",
        "                try:\n",
        "                    fid, obj_id = int(parts[0]), int(parts[1])\n",
        "\n",
        "                    # Filtro palla\n",
        "                    if obj_id in ids_to_ignore:\n",
        "                        removed_balls += 1\n",
        "                        continue\n",
        "\n",
        "                    x, y, w, h = float(parts[2]), float(parts[3]), float(parts[4]), float(parts[5])\n",
        "                    xc = max(0.0, min(1.0, (x + w/2) / W))\n",
        "                    yc = max(0.0, min(1.0, (y + h/2) / H))\n",
        "                    wn = max(0.0, min(1.0, w / W))\n",
        "                    hn = max(0.0, min(1.0, h / H))\n",
        "\n",
        "                    label_str = f\"0 {xc:.6f} {yc:.6f} {wn:.6f} {hn:.6f}\"\n",
        "                    if fid not in anns: anns[fid] = []\n",
        "                    anns[fid].append(label_str)\n",
        "                    total_boxes += 1\n",
        "                except ValueError: continue\n",
        "\n",
        "        images = sorted([os.path.basename(x) for x in images])\n",
        "        for fname in tqdm(images, desc=f\"{seq_name}\", leave=False):\n",
        "            try: fid = int(fname.split('.')[0])\n",
        "            except: continue\n",
        "\n",
        "            new_name = f\"{seq_name}_{fname}\"\n",
        "            shutil.copy(os.path.join(img_folder, fname), os.path.join(img_dest_path, new_name))\n",
        "\n",
        "            txt_name = os.path.splitext(new_name)[0] + \".txt\"\n",
        "            with open(os.path.join(lbl_dest_path, txt_name), 'w') as f_out:\n",
        "                if fid in anns:\n",
        "                    for line in anns[fid]:\n",
        "                        f_out.write(line + '\\n')\n",
        "            total_frames += 1\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"‚úÖ DATASET FORMATTATO CON SUCCESSO\")\n",
        "    print(f\"üìÅ Immagini processate: {total_frames}\")\n",
        "    print(f\"üì¶ Box salvati (Player/Ref/GK): {total_boxes}\")\n",
        "    print(f\"‚öΩ Box PALLA filtrati e rimossi: {removed_balls}\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "# Esegui la conversione solo se la cartella RAW esiste ed √® stata estratta\n",
        "if os.path.exists(RAW_DATA_DIR) and os.path.isdir(RAW_DATA_DIR):\n",
        "    # Controllo rapido per evitare di lanciare la funzione se la cartella √® vuota\n",
        "    if any(os.scandir(RAW_DATA_DIR)):\n",
        "        convert_soccernet_clean(RAW_DATA_DIR, YOLO_DATA_DIR)"
      ],
      "metadata": {
        "id": "PfqMfrnrqHQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVDSyxOJX2En"
      },
      "source": [
        "#YOLOv11M Standard\n",
        "Addestramento del backbone"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8SstoYsc614"
      },
      "source": [
        "Visualizzazione labels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 5. DATASET VISUALIZATION (Sanity Check)\n",
        "# ==========================================\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "def visualize_sample(image_filename, split='train', dataset_root='./dataset_yolo'):\n",
        "    \"\"\"Visualizza un'immagine e le sue bounding box di ground truth.\"\"\"\n",
        "    if not image_filename.endswith('.jpg'): image_filename += '.jpg'\n",
        "    image_name_no_ext = image_filename.replace('.jpg', '')\n",
        "    txt_filename = image_name_no_ext + '.txt'\n",
        "\n",
        "    img_path = os.path.join(dataset_root, 'images', split, image_filename)\n",
        "    lbl_path = os.path.join(dataset_root, 'labels', split, txt_filename)\n",
        "\n",
        "    if not os.path.exists(img_path):\n",
        "        print(f\"‚ùå Immagine non trovata: {img_path}\")\n",
        "        return\n",
        "\n",
        "    img = cv2.imread(img_path)\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    h_img, w_img, _ = img.shape\n",
        "\n",
        "    if os.path.exists(lbl_path):\n",
        "        with open(lbl_path, 'r') as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split()\n",
        "                class_id, x_center_norm, y_center_norm, width_norm, height_norm = map(float, parts[:5])\n",
        "\n",
        "                # Conversione YOLO -> Pixel\n",
        "                x_center, y_center = x_center_norm * w_img, y_center_norm * h_img\n",
        "                box_w, box_h = width_norm * w_img, height_norm * h_img\n",
        "\n",
        "                x1, y1 = int(x_center - box_w / 2), int(y_center - box_h / 2)\n",
        "                x2, y2 = int(x_center + box_w / 2), int(y_center + box_h / 2)\n",
        "\n",
        "                cv2.rectangle(img_rgb, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "                cv2.putText(img_rgb, \"Player\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.imshow(img_rgb)\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Sample: {image_name_no_ext} | Split: {split}\")\n",
        "    plt.show()\n",
        "\n",
        "visualize_sample('SNMOT-060_000001.jpg', split='train')"
      ],
      "metadata": {
        "id": "WwGrwb3N0ab4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzWliQMrXiJw"
      },
      "source": [
        "Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfHBo4UsXg8u"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 6. YOLO CONFIGURATION & TRAINING\n",
        "# ==========================================\n",
        "import yaml\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# 1. Creazione file YAML per il dataset\n",
        "dataset_config = {\n",
        "    'path': './dataset_yolo',  # Percorso relativo radice\n",
        "    'train': 'images/train',\n",
        "    'val': 'images/test',      # Usiamo test come validazione se non c'√® val\n",
        "    'names': {0: 'player'}\n",
        "}\n",
        "\n",
        "yaml_filename = 'soccernet.yaml'\n",
        "with open(yaml_filename, 'w') as f:\n",
        "    yaml.dump(dataset_config, f, default_flow_style=False)\n",
        "print(f\"‚úÖ Configurazione {yaml_filename} creata!\")\n",
        "\n",
        "# 2. Avvio Addestramento\n",
        "print(\"\\nüöÄ Avvio Training YOLOv11m...\")\n",
        "model = YOLO('yolo11m.pt')\n",
        "\n",
        "# Addestramento con salvataggio locale nella cartella './runs'\n",
        "results = model.train(\n",
        "    data=yaml_filename,\n",
        "    imgsz=1088,                # Risoluzione alta per il calcio\n",
        "    epochs=80,\n",
        "    batch=8,                   # Abbassare se OOM\n",
        "    rect=False,\n",
        "    project='./runs',          # <--- Modificato: Salvataggio locale\n",
        "    name='soccernet_train',    # <--- Modificato: Nome pulito\n",
        "    cache=True,\n",
        "    workers=4,\n",
        "    optimizer='Adam',\n",
        "    lr0=0.001,\n",
        "    cos_lr=True,\n",
        "    patience=20,\n",
        "    # Augmentation strategy\n",
        "    mosaic=1.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, fliplr=0.5,\n",
        "    scale=0.5, blur=0.1, degrees=0.0, mixup=0.0, copy_paste=0.0, close_mosaic=10\n",
        ")\n",
        "\n",
        "print(\"üèÜ Training completato con successo!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8nNH-eAjh0P"
      },
      "source": [
        "Validazione e metriche"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjr71UiMjj4e",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 7. VALIDATION & INFERENCE\n",
        "# ==========================================\n",
        "import os\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Percorsi relativi aggiornati in base alla cella di training\n",
        "best_weights = './runs/soccernet_train/weights/best.pt'\n",
        "yaml_path = 'soccernet.yaml'\n",
        "test_images_path = './dataset_yolo/images/test'\n",
        "\n",
        "if not os.path.exists(best_weights):\n",
        "    print(f\"‚ùå Errore: Pesi non trovati in {best_weights}. Eseguire prima il training.\")\n",
        "else:\n",
        "    print(f\"‚úÖ Modello caricato da: {best_weights}\")\n",
        "    model = YOLO(best_weights)\n",
        "\n",
        "    # --- VALIDAZIONE ---\n",
        "    print(\"\\nüìä Avvio Validazione sul set di Test...\")\n",
        "    metrics = model.val(\n",
        "        data=yaml_path,\n",
        "        split='val',\n",
        "        imgsz=1088,\n",
        "        batch=8,\n",
        "        augment=True,\n",
        "        conf=0.25,\n",
        "        iou=0.6,\n",
        "        plots=True,\n",
        "        save_json=True,\n",
        "        project='./runs',\n",
        "        name='soccernet_val'\n",
        "    )\n",
        "\n",
        "    print(\"\\nüèÜ REPORT METRICHE:\")\n",
        "    print(f\"üéØ Precision:  {metrics.box.mp:.4f}\")\n",
        "    print(f\"üì° Recall:     {metrics.box.mr:.4f}\")\n",
        "    print(f\"üìè mAP@50:     {metrics.box.map50:.4f}\")\n",
        "    print(f\"üìê mAP@50-95:  {metrics.box.map:.4f}\")\n",
        "\n",
        "    # --- INFERENZA VISIVA ---\n",
        "    print(\"\\nüé® Avvio generazione predizioni visive (Inference)...\")\n",
        "    # Non serve il ciclo for se usiamo il parametro stream=False.\n",
        "    # YOLO salva tutto automaticamente grazie a save=True.\n",
        "    predictions = model.predict(\n",
        "        source=test_images_path,\n",
        "        imgsz=1088,\n",
        "        conf=0.25,\n",
        "        iou=0.45,\n",
        "        max_det=50,\n",
        "        line_width=3,\n",
        "        show_labels=True,\n",
        "        show_conf=True,\n",
        "        augment=True,\n",
        "        save=True,\n",
        "        project='./runs',\n",
        "        name='soccernet_predictions'\n",
        "    )\n",
        "\n",
        "    print(\"\\n‚úÖ Predizioni salvate! Puoi trovarle nella cartella './runs/soccernet_predictions'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYnEf0eSUZ7Q"
      },
      "source": [
        "# Fine tuning YOLO11 per occlusioni e blur"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gO1QIJ2AgJGd"
      },
      "source": [
        "Train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWpv6haOgKiE",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 7. PHASE 2: ADVANCED FINE-TUNING (Motion Blur & Occlusion Handling)\n",
        "# ==========================================\n",
        "# Dopo un primo addestramento, implementiamo una pipeline aggressiva per\n",
        "# risolvere i casi critici: giocatori in corsa (motion blur), telecamere\n",
        "# fuori fuoco e occlusioni tra giocatori (mixup ed erasing).\n",
        "\n",
        "import albumentations as A\n",
        "from ultralytics import YOLO\n",
        "from ultralytics.data import augment\n",
        "\n",
        "# --- 1. Patching Ultralytics con Albumentations Custom ---\n",
        "def custom_albumentations(self, p=1.0, **kwargs):\n",
        "    self.p = p\n",
        "    self.transform = None\n",
        "    self.contains_spatial = False\n",
        "    try:\n",
        "        # Pipeline aggressiva specifica per SoccerNet / Sports Tracking\n",
        "        self.transform = A.Compose([\n",
        "            # --- BLUR (Anti-Ghosting per movimenti rapidi) ---\n",
        "            A.OneOf([\n",
        "                A.MotionBlur(blur_limit=(15, 35), p=0.6),\n",
        "                A.ZoomBlur(max_factor=1.3, step_factor=0.02, p=0.3),\n",
        "            ], p=0.6),\n",
        "\n",
        "            # --- LUCE E QUALIT√Ä (Riprese notturne o artefatti) ---\n",
        "            A.CLAHE(clip_limit=3.0, tile_grid_size=(8, 8), p=0.3),\n",
        "            A.GaussNoise(var_limit=(20.0, 80.0), p=0.2),\n",
        "            A.ImageCompression(quality_lower=60, quality_upper=90, p=0.2),\n",
        "\n",
        "            # --- COLORE E OCCLUSIONI PARZIALI ---\n",
        "            A.RandomBrightnessContrast(brightness_limit=0.25, contrast_limit=0.25, p=0.4),\n",
        "            A.CoarseDropout(max_holes=8, max_height=32, max_width=32, min_holes=2, p=0.2),\n",
        "        ], bbox_params=A.BboxParams(format='yolo'))\n",
        "    except ImportError:\n",
        "        print(\"‚ö†Ô∏è Albumentations non installato o errore nella composizione.\")\n",
        "\n",
        "# Applichiamo la patch iniettandola nella classe di Ultralytics\n",
        "augment.Albumentations.__init__ = custom_albumentations\n",
        "print(\"‚úÖ Custom Albumentations Pipeline iniettata con successo!\")\n",
        "\n",
        "# --- 2. Avvio Fine-Tuning ---\n",
        "# Partiamo dai pesi migliori ottenuti nel training precedente\n",
        "previous_best_weights = './runs/soccernet_train/weights/best.pt'\n",
        "\n",
        "print(f\"\\nüöÄ Avvio Fine-Tuning partendo da: {previous_best_weights}\")\n",
        "model_finetune = YOLO(previous_best_weights)\n",
        "\n",
        "# Addestramento mirato (Fine-Tuning)\n",
        "results_finetune = model_finetune.train(\n",
        "    data='soccernet.yaml',\n",
        "    epochs=100,\n",
        "    imgsz=1088,\n",
        "    batch=16,          # Se va in Out Of Memory, abbassa a 8\n",
        "    patience=20,\n",
        "    workers=4,\n",
        "    cache=True,\n",
        "\n",
        "    # Ottimizzazione specifica per Fine-Tuning\n",
        "    optimizer='adamw', # AdamW gestisce meglio il weight decay\n",
        "    lr0=0.002,         # Learning rate pi√π basso per non distruggere i pesi precedenti\n",
        "    cos_lr=True,\n",
        "\n",
        "    # --- AUGMENTATION GEOMETRICHE & COLLISIONI ---\n",
        "    mosaic=1.0,\n",
        "    mixup=0.25,        # CRUCIALE: Insegna che due giocatori possono sovrapporsi\n",
        "    erasing=0.5,       # CRUCIALE: Simula occlusioni parziali (gambe tagliate, ecc.)\n",
        "    scale=0.4,\n",
        "    degrees=0.0,\n",
        "    fliplr=0.5,\n",
        "    flipud=0.0,\n",
        "    close_mosaic=10,   # Stabilizza le ultime 10 epoche su immagini reali pure\n",
        "\n",
        "    # Salvataggio\n",
        "    project='./runs',\n",
        "    name='soccernet_finetune_blur'\n",
        ")\n",
        "\n",
        "print(\"üèÜ Fine-Tuning completato! Il modello √® ora robusto a blur e occlusioni.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RT-DETR L"
      ],
      "metadata": {
        "id": "Lq2MSJGVq003"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_p2bOvtrIyn"
      },
      "source": [
        "Configurazione dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqsVuZtzrIyo",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 1. RT-DETR CONFIGURATION & SETUP\n",
        "# ==========================================\n",
        "import yaml\n",
        "import os\n",
        "\n",
        "# Creazione del file YAML per RT-DETR con percorsi relativi\n",
        "dataset_config = {\n",
        "    'path': './dataset_rtdetr',  # Assicurati di avere i dati in questa cartella\n",
        "    'train': 'images/train',\n",
        "    'val': 'images/test',        # Usiamo il test set come validazione\n",
        "    'names': {0: 'player'}\n",
        "}\n",
        "\n",
        "yaml_filename = 'soccernet_rtdetr.yaml'\n",
        "with open(yaml_filename, 'w') as f:\n",
        "    yaml.dump(dataset_config, f, default_flow_style=False)\n",
        "\n",
        "print(f\"‚úÖ File {yaml_filename} creato con successo!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "definizione classi e augmentation custom\n",
        "\n"
      ],
      "metadata": {
        "id": "6Yb9DknnAAYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 2. ADVANCED CROWD AUGMENTATION ENGINE\n",
        "# ==========================================\n",
        "# Implementazione di una pipeline di Copy-Paste intelligente:\n",
        "# Estrae giocatori esistenti e li incolla casualmente SOLO sull'erba (tramite maschera convessa),\n",
        "# simulando situazioni di \"Crowd\" (affollamento) e occlusioni tipiche del calcio.\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "import albumentations as A\n",
        "import ultralytics\n",
        "from ultralytics.data import augment\n",
        "from ultralytics.utils.instance import Instances\n",
        "\n",
        "def get_field_mask_fast(image_bgr):\n",
        "    \"\"\"Genera una maschera convessa dell'erba per incollare i giocatori in zone sicure.\"\"\"\n",
        "    h, w = image_bgr.shape[:2]\n",
        "    hsv = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2HSV)\n",
        "    mask = cv2.inRange(hsv, (35, 50, 50), (85, 255, 255))\n",
        "\n",
        "    kernel_open = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel_open)\n",
        "\n",
        "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    if not contours: return np.zeros((h, w), dtype=np.uint8)\n",
        "\n",
        "    largest_cnt = max(contours, key=cv2.contourArea)\n",
        "    if cv2.contourArea(largest_cnt) < (h * w * 0.05):\n",
        "        return np.zeros((h, w), dtype=np.uint8)\n",
        "\n",
        "    hull = cv2.convexHull(largest_cnt)\n",
        "    safe_mask = np.zeros((h, w), dtype=np.uint8)\n",
        "    cv2.drawContours(safe_mask, [hull], -1, 255, thickness=cv2.FILLED)\n",
        "\n",
        "    erosion_size = int(h * 0.05)\n",
        "    kernel_erode = cv2.getStructuringElement(cv2.MORPH_RECT, (erosion_size, erosion_size))\n",
        "    return cv2.erode(safe_mask, kernel_erode, iterations=1)\n",
        "\n",
        "def apply_soccer_copy_paste_crowd(image, bboxes_xyxy, classes, p=0.5):\n",
        "    \"\"\"Incolla cloni di giocatori esistenti per aumentare la densit√† della folla.\"\"\"\n",
        "    if random.random() > p or len(bboxes_xyxy) == 0:\n",
        "        return image, bboxes_xyxy, classes\n",
        "\n",
        "    h_img, w_img = image.shape[:2]\n",
        "    field_mask = get_field_mask_fast(image)\n",
        "\n",
        "    if cv2.countNonZero(field_mask) < (h_img * w_img * 0.05):\n",
        "        return image, bboxes_xyxy, classes\n",
        "\n",
        "    out_bboxes, out_classes = list(bboxes_xyxy), list(classes)\n",
        "    num_to_paste = random.randint(5, 15)\n",
        "    valid_indices = list(range(len(bboxes_xyxy)))\n",
        "\n",
        "    for _ in range(num_to_paste):\n",
        "        if not valid_indices: break\n",
        "        idx = random.choice(valid_indices)\n",
        "        x1, y1, x2, y2 = map(int, bboxes_xyxy[idx])\n",
        "        cls_id = classes[idx]\n",
        "\n",
        "        x1, y1 = max(0, x1), max(0, y1)\n",
        "        x2, y2 = min(w_img, x2), min(h_img, y2)\n",
        "        w_obj, h_obj = x2 - x1, y2 - y1\n",
        "\n",
        "        if w_obj <= 5 or h_obj <= 5: continue\n",
        "        patch = image[y1:y2, x1:x2].copy()\n",
        "\n",
        "        for _ in range(15): # Max 15 tentativi di posizionamento\n",
        "            rx = random.randint(0, w_img - w_obj)\n",
        "            ry = random.randint(0, h_img - h_obj)\n",
        "            feet_x, feet_y = min(rx + w_obj // 2, w_img - 1), min(ry + h_obj, h_img - 1)\n",
        "\n",
        "            if field_mask[feet_y, feet_x] > 0:\n",
        "                image[ry:ry+h_obj, rx:rx+w_obj] = patch\n",
        "                out_bboxes.append([rx, ry, rx+w_obj, ry+h_obj])\n",
        "                out_classes.append(cls_id)\n",
        "                break\n",
        "\n",
        "    return image, np.array(out_bboxes, dtype=np.float32), np.array(out_classes, dtype=np.float32)\n",
        "\n",
        "class CustomSoccerAugmentV5_Final:\n",
        "    \"\"\"Classe custom per iniettare Copy-Paste e trasformazioni Albumentations.\"\"\"\n",
        "    def __init__(self, p=1.0):\n",
        "        self.p = p\n",
        "        self.transform = A.Compose([\n",
        "            A.OneOf([A.MotionBlur(blur_limit=(10, 25), p=0.6), A.ZoomBlur(max_factor=1.15, step_factor=0.02, p=0.3)], p=0.5),\n",
        "            A.GaussNoise(var_limit=(20.0, 60.0), p=0.1),\n",
        "            A.RandomBrightnessContrast(p=0.4),\n",
        "            A.Affine(shear={'x': (-10, 10)}, p=0.3),\n",
        "        ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels'], min_visibility=0.1))\n",
        "\n",
        "    def __call__(self, labels):\n",
        "        img = labels.get('img')\n",
        "        if img is None or 'instances' not in labels: return labels\n",
        "\n",
        "        h, w = img.shape[:2]\n",
        "        labels['shape'] = (h, w)\n",
        "        orig_instances = labels['instances']\n",
        "\n",
        "        try:\n",
        "            bboxes_xyxy, classes = labels['instances'].bboxes, labels['cls'].squeeze()\n",
        "            if classes.ndim == 0: classes = classes.reshape(-1)\n",
        "\n",
        "            img_aug, aug_bboxes, aug_classes = apply_soccer_copy_paste_crowd(img, bboxes_xyxy, classes, p=0.6)\n",
        "\n",
        "            if not np.isfinite(aug_bboxes).all(): raise ValueError(\"NaN in CopyPaste boxes\")\n",
        "\n",
        "            safe_bboxes, safe_classes = [], []\n",
        "            h_aug, w_aug = img_aug.shape[:2]\n",
        "            for box, cls in zip(aug_bboxes, aug_classes):\n",
        "                x1, y1, x2, y2 = np.clip(box, 0, [w_aug-1, h_aug-1, w_aug-1, h_aug-1])\n",
        "                if (x2 > x1 + 2) and (y2 > y1 + 2):\n",
        "                    safe_bboxes.append([x1, y1, x2, y2])\n",
        "                    safe_classes.append(cls)\n",
        "\n",
        "            if not safe_bboxes: raise ValueError(\"No boxes left after clipping\")\n",
        "\n",
        "            transformed = self.transform(image=img_aug, bboxes=safe_bboxes, class_labels=safe_classes)\n",
        "            final_img = transformed['image']\n",
        "            final_boxes = np.array(transformed['bboxes'], dtype=np.float32)\n",
        "\n",
        "            if not np.isfinite(final_boxes).all(): raise ValueError(\"NaN after Albumentations\")\n",
        "\n",
        "            labels['img'] = final_img\n",
        "            labels['cls'] = np.array(transformed['class_labels'], dtype=np.float32).reshape(-1, 1)\n",
        "            labels['shape'] = final_img.shape[:2]\n",
        "\n",
        "            new_inst = Instances(final_boxes, segments=np.zeros((0, 2), dtype=np.float32), bbox_format=\"xyxy\", normalized=False)\n",
        "            new_inst.shape = final_img.shape[:2]\n",
        "            labels['instances'] = new_inst\n",
        "            labels['bboxes'] = final_boxes\n",
        "            return labels\n",
        "\n",
        "        except Exception:\n",
        "            labels['instances'] = orig_instances\n",
        "            labels['shape'] = (h, w)\n",
        "            return labels\n",
        "\n",
        "class AlbumentationsHijack:\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self.crowd_transform = CustomSoccerAugmentV5_Final(p=0.7)\n",
        "    def __call__(self, labels):\n",
        "        return self.crowd_transform(labels)\n",
        "\n",
        "# Iniezione della classe custom all'interno del motore di Ultralytics\n",
        "ultralytics.data.augment.Albumentations = AlbumentationsHijack\n",
        "print(\"‚úÖ HIJACK ATTIVO: Extreme Crowd Augmentation V2 inizializzata con successo!\")"
      ],
      "metadata": {
        "id": "heq0z5uWACKx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1f351eb-aab1-4703-bdb8-ecc9a3f60752"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ HIJACK ATTIVO: Usa CustomSoccerAugmentV4_Crowd_Final\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "train"
      ],
      "metadata": {
        "id": "FezM4EwJ4Hmr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYlZQLOUrIyo",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 3. RT-DETR TRAINING\n",
        "# ==========================================\n",
        "from ultralytics import YOLO\n",
        "\n",
        "print(\"üöÄ Avvio Training RT-DETR-Large...\")\n",
        "\n",
        "# Ultralytics supporta RT-DETR nativamente usando la stessa API di YOLO\n",
        "model = YOLO('rtdetr-l.pt')\n",
        "\n",
        "results = model.train(\n",
        "    data='soccernet_rtdetr.yaml',\n",
        "    epochs=100,\n",
        "    imgsz=1088,\n",
        "    batch=8,           # Assicurati di avere abbastanza VRAM per un Transformer\n",
        "    patience=20,\n",
        "\n",
        "    # --- AUGMENTATION STRUTTURALI ---\n",
        "    mosaic=1.0,\n",
        "    mixup=0.2,         # Fondamentale per gestire collisioni nel Crowd\n",
        "    erasing=0.4,       # Simula occlusioni\n",
        "    scale=0.5,\n",
        "    fliplr=0.5,\n",
        "    flipud=0.0,\n",
        "\n",
        "    # --- PERFORMANCE & OTTIMIZZAZIONE ---\n",
        "    workers=8,\n",
        "    cache='disk',      # Usa 'ram' se hai molta memoria di sistema\n",
        "    optimizer='adamW', # AdamW √® fortemente raccomandato per i Transformer (rispetto a SGD)\n",
        "    lr0=0.001,\n",
        "    lrf=0.01,\n",
        "    close_mosaic=15,   # Stabilizzazione finale su dati reali\n",
        "\n",
        "    # --- SALVATAGGIO LOCALE ---\n",
        "    project='./runs/RtDetr',\n",
        "    name='Crowd_Blur_Extreme_Tuned'\n",
        ")\n",
        "\n",
        "print(\"üèÜ Training RT-DETR completato!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIrYuMDkrIyp"
      },
      "source": [
        "Validazione e metriche"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9MtYh58rIyp",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 4. RT-DETR VALIDATION & METRICS\n",
        "# ==========================================\n",
        "from ultralytics import YOLO\n",
        "import os\n",
        "\n",
        "# Percorso relativo generato automaticamente dalla cella precedente\n",
        "weights_path = './runs/RtDetr/Crowd_Blur_Extreme_Tuned/weights/best.pt'\n",
        "yaml_path = 'soccernet_rtdetr.yaml'\n",
        "\n",
        "if os.path.exists(weights_path):\n",
        "    print(f\"‚úÖ Caricamento modello RT-DETR da: {weights_path}\")\n",
        "    model = YOLO(weights_path)\n",
        "\n",
        "    print(\"üìä Avvio Validazione sul Test Set...\")\n",
        "\n",
        "    metrics = model.val(\n",
        "        data=yaml_path,\n",
        "        split='val',\n",
        "        imgsz=1088,\n",
        "        batch=8,\n",
        "        augment=True,\n",
        "        conf=0.25,\n",
        "        iou=0.6,\n",
        "        plots=True,\n",
        "        save_json=True,\n",
        "        project='./runs/RtDetr',\n",
        "        name='rtdetr_metrics',\n",
        "        exist_ok=True\n",
        "    )\n",
        "\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(\"üèÜ REPORT DI VALIDAZIONE RT-DETR\")\n",
        "    print(\"=\"*40)\n",
        "    print(f\"üéØ Precision (Media):  {metrics.box.mp:.4f}\")\n",
        "    print(f\"üì° Recall (Media):     {metrics.box.mr:.4f}\")\n",
        "    print(f\"üìè mAP @ 50%:          {metrics.box.map50:.4f}\")\n",
        "    print(f\"üìê mAP @ 50-95%:       {metrics.box.map:.4f}\")\n",
        "\n",
        "    if metrics.box.nc > 1:\n",
        "        print(\"\\n--- Dettaglio per Classe ---\")\n",
        "        for i, c in enumerate(metrics.names.values()):\n",
        "            print(f\"   {c}: P={metrics.box.p[i]:.3f}, R={metrics.box.r[i]:.3f}\")\n",
        "\n",
        "    print(\"\\nüìÇ Grafici e metriche salvati in: ./runs/RtDetr/rtdetr_metrics\")\n",
        "else:\n",
        "    print(f\"‚ùå Errore: Il file pesi non esiste in {weights_path}. Eseguire prima il training.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kHNWERj2UG0"
      },
      "source": [
        "# Tracking - Conversione del dataset reid"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 1. RE-ID DATASET PREPARATION & CLEANING\n",
        "# ==========================================\n",
        "# In questa fase estraiamo i metadati dal JSON di SoccerNet, creiamo ID univoci (Hash MD5)\n",
        "# per i giocatori attraverso diverse telecamere e filtriamo le identit√† con troppe poche\n",
        "# immagini, un passaggio cruciale per generare batch validi per la Triplet Loss.\n",
        "\n",
        "import os\n",
        "import json\n",
        "import shutil\n",
        "import hashlib\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- CONFIGURAZIONE PERCORSI RELATIVI ---\n",
        "RAW_DATASET_ROOT = Path('./dataset_reid_raw')\n",
        "CLEAN_DATASET_ROOT = Path('./dataset_reid_clean')\n",
        "\n",
        "def get_unique_pid(relative_path, clazz, player_id):\n",
        "    \"\"\"Genera un ID univoco crittografico (MD5) basato su Partita + Squadra + PlayerID\"\"\"\n",
        "    raw_string = f\"{relative_path}_{clazz}_{player_id}\"\n",
        "    return hashlib.md5(raw_string.encode()).hexdigest()[:10]\n",
        "\n",
        "def construct_filename(item):\n",
        "    \"\"\"Ricostruisce il filename originale dai metadati JSON\"\"\"\n",
        "    return f\"{item['bbox_idx']}-{item['action_idx']}-{item['person_uid']}-{item['frame_idx']}-{item['clazz']}-{item['id']}-{item['UAI']}-{item['height']}x{item['width']}.png\"\n",
        "\n",
        "def restructure_reid_data(split='train'):\n",
        "    \"\"\"Estrae le immagini usando il JSON e le raggruppa in cartelle per ID univoco.\"\"\"\n",
        "    source_split_dir = RAW_DATASET_ROOT / split\n",
        "    target_split_dir = CLEAN_DATASET_ROOT / split\n",
        "    json_path = source_split_dir / 'bbox_info.json' # Adatta il nome se necessario\n",
        "\n",
        "    if not json_path.exists():\n",
        "        print(f\"‚ö†Ô∏è JSON non trovato per lo split '{split}' in {json_path}. Salto.\")\n",
        "        return\n",
        "\n",
        "    if target_split_dir.exists():\n",
        "        shutil.rmtree(target_split_dir)\n",
        "    target_split_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    with open(json_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Adattamento per strutture JSON piatte o annidate\n",
        "    items = list(data.values()) if isinstance(data, dict) else data\n",
        "\n",
        "    unique_identities_map = {}\n",
        "    pid_counter = 0\n",
        "    moved = 0\n",
        "\n",
        "    print(f\"\\nüöÄ Ristrutturazione {split.upper()} set in corso...\")\n",
        "    for item in tqdm(items, desc=f\"Processing {split}\"):\n",
        "        try:\n",
        "            player_id = str(item.get('id', 'None'))\n",
        "            if player_id == \"None\": continue # Scartiamo le identit√† non note\n",
        "\n",
        "            unique_hash = get_unique_pid(item['relative_path'], item['clazz'], player_id)\n",
        "            if unique_hash not in unique_identities_map:\n",
        "                unique_identities_map[unique_hash] = f\"{pid_counter:05d}\"\n",
        "                pid_counter += 1\n",
        "\n",
        "            pid_folder = unique_identities_map[unique_hash]\n",
        "            filename = construct_filename(item)\n",
        "\n",
        "            # Cerca il file sorgente (gestendo possibili discrepanze di path)\n",
        "            src_path = source_split_dir / item['relative_path'] / filename\n",
        "            if not src_path.exists():\n",
        "                src_path = source_split_dir / split / item['relative_path'] / filename # Fallback nested\n",
        "\n",
        "            if src_path.exists():\n",
        "                dest_dir = target_split_dir / pid_folder\n",
        "                dest_dir.mkdir(exist_ok=True)\n",
        "                shutil.copy2(src_path, dest_dir / filename)\n",
        "                moved += 1\n",
        "\n",
        "        except Exception as e: continue\n",
        "\n",
        "    print(f\"‚úÖ {split.upper()}: Create {pid_counter} identit√†, {moved} immagini spostate.\")\n",
        "\n",
        "def clean_reid_dataset(target_dir, min_images=4):\n",
        "    \"\"\"\n",
        "    Filtro vitale per Triplet Loss: scarta identit√† con meno di `min_images`.\n",
        "    La Triplet Loss richiede Anchor, Positive e Negative, quindi servono pi√π scatti per ID.\n",
        "    \"\"\"\n",
        "    if not target_dir.exists(): return\n",
        "\n",
        "    trash_dir = CLEAN_DATASET_ROOT / 'trash_bin'\n",
        "    trash_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    classes = [d for d in os.listdir(target_dir) if os.path.isdir(target_dir / d)]\n",
        "    moved_count = 0\n",
        "\n",
        "    print(f\"\\nüßπ Pulizia {target_dir.name}: Rimuovo classi con < {min_images} immagini...\")\n",
        "    for class_name in tqdm(classes, desc=\"Filtering IDs\"):\n",
        "        class_path = target_dir / class_name\n",
        "        imgs = [f for f in os.listdir(class_path) if f.lower().endswith(('.png', '.jpg'))]\n",
        "\n",
        "        if len(imgs) < min_images:\n",
        "            shutil.move(str(class_path), str(trash_dir / f\"{target_dir.name}_{class_name}\"))\n",
        "            moved_count += 1\n",
        "\n",
        "    print(f\"üóëÔ∏è Scartate {moved_count} classi. Rimaste: {len(classes) - moved_count} classi valide per il training.\")\n",
        "\n",
        "# --- ESECUZIONE DELLA PIPELINE ---\n",
        "if RAW_DATASET_ROOT.exists():\n",
        "    for current_split in ['train', 'valid', 'test']:\n",
        "        restructure_reid_data(current_split)\n",
        "        clean_reid_dataset(CLEAN_DATASET_ROOT / current_split, min_images=4)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Cartella dati RAW non trovata. Inserire i dati in './dataset_reid_raw' per eseguire.\")"
      ],
      "metadata": {
        "id": "4OGSizT8IjD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BDY8ab6KyWq"
      },
      "source": [
        "# Organizzazione del dataset di tracking per Re-ID"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 1. RE-ID DATASET GENERATION (Crops Extraction)\n",
        "# ==========================================\n",
        "import os\n",
        "import cv2\n",
        "from tqdm.notebook import tqdm\n",
        "from collections import defaultdict\n",
        "import shutil\n",
        "\n",
        "# --- CONFIGURAZIONE PERCORSI RELATIVI ---\n",
        "SOURCE_ROOT = './dataset_soccernet_yolo'\n",
        "OUTPUT_ROOT = './soccernet_reid_robust'\n",
        "\n",
        "TRAIN_SEQUENCES = ['SNMOT-060', 'SNMOT-065', 'SNMOT-070', 'SNMOT-097', 'SNMOT-107']\n",
        "TEST_SEQUENCES = ['SNMOT-116', 'SNMOT-129', 'SNMOT-130', 'SNMOT-141']\n",
        "\n",
        "# Variabili globali per tracciare le identit√† univoche attraverso le sequenze\n",
        "global_identity_map = {}\n",
        "next_global_pid = 0\n",
        "\n",
        "def parse_gameinfo(ini_path):\n",
        "    \"\"\"Estrae GameID e Tempo di gioco per creare identit√† univoche.\"\"\"\n",
        "    game_id, half_period = None, \"1\"\n",
        "    tracklet_map = {}\n",
        "    with open(ini_path, 'r') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if 'gameID=' in line: game_id = line.split('=')[1].strip()\n",
        "            if 'gameTimeStart=' in line:\n",
        "                time_val = line.split('=')[1].strip()\n",
        "                half_period = time_val.split(' - ')[0].strip() if ' - ' in time_val else time_val[0]\n",
        "            if line.startswith('trackletID_'):\n",
        "                parts = line.split('=')\n",
        "                local_id = int(parts[0].split('_')[1])\n",
        "                tracklet_map[local_id] = parts[1].strip()\n",
        "    return game_id, half_period, tracklet_map\n",
        "\n",
        "def extract_crops(sequences, split_type='train', gallery_freq=10):\n",
        "    \"\"\"Estrae i crop dei giocatori e li salva in Train o Query/Gallery.\"\"\"\n",
        "    global next_global_pid\n",
        "    split_source = os.path.join(SOURCE_ROOT, 'train' if split_type == 'train' else 'test')\n",
        "\n",
        "    print(f\"\\nüöÄ Inizio estrazione {split_type.upper()} SET...\")\n",
        "\n",
        "    for seq_name in sequences:\n",
        "        seq_path = os.path.join(split_source, seq_name)\n",
        "        ini_path, gt_path = os.path.join(seq_path, 'gameinfo.ini'), os.path.join(seq_path, 'gt', 'gt.txt')\n",
        "        img_dir = os.path.join(seq_path, 'img1')\n",
        "\n",
        "        if not os.path.exists(ini_path) or not os.path.exists(gt_path): continue\n",
        "\n",
        "        game_id, half_period, tracklet_map = parse_gameinfo(ini_path)\n",
        "        if not game_id: continue\n",
        "\n",
        "        # Mapping Locale -> Globale\n",
        "        local_to_global = {}\n",
        "        for local_id, label in tracklet_map.items():\n",
        "            if 'ball' in label.lower(): continue\n",
        "            identity_key = (game_id, half_period, label)\n",
        "            if identity_key not in global_identity_map:\n",
        "                global_identity_map[identity_key] = next_global_pid\n",
        "                next_global_pid += 1\n",
        "            local_to_global[local_id] = global_identity_map[identity_key]\n",
        "\n",
        "        # Lettura GT\n",
        "        frame_data = defaultdict(list)\n",
        "        with open(gt_path, 'r') as f:\n",
        "            for line in f:\n",
        "                p = line.strip().split(',')\n",
        "                frame, obj_id, x, y, w, h = int(p[0]), int(p[1]), float(p[2]), float(p[3]), float(p[4]), float(p[5])\n",
        "                if obj_id in local_to_global:\n",
        "                    frame_data[frame].append((obj_id, int(x), int(y), int(w), int(h)))\n",
        "\n",
        "        # Estrazione Immagini\n",
        "        id_saved_count = defaultdict(int)\n",
        "        for frame_idx in tqdm(sorted(frame_data.keys()), desc=f\"Cropping {seq_name}\", leave=False):\n",
        "            img_path = os.path.join(img_dir, f\"{frame_idx:06d}.jpg\")\n",
        "            if not os.path.exists(img_path): continue\n",
        "\n",
        "            image = cv2.imread(img_path)\n",
        "            if image is None: continue\n",
        "            h_img, w_img, _ = image.shape\n",
        "\n",
        "            for (local_id, x, y, w, h) in frame_data[frame_idx]:\n",
        "                if w < 10 or h < 10: continue\n",
        "                x1, y1, x2, y2 = max(0, x), max(0, y), min(w_img, int(x+w)), min(h_img, int(y+h))\n",
        "                crop = image[y1:y2, x1:x2]\n",
        "\n",
        "                pid = local_to_global[local_id]\n",
        "                pid_str = f\"{pid:05d}\"\n",
        "                save_name = f\"{seq_name}_{frame_idx}_{local_id}.jpg\"\n",
        "\n",
        "                if split_type == 'train':\n",
        "                    save_dir = os.path.join(OUTPUT_ROOT, 'train', pid_str)\n",
        "                else: # Gestione Query/Gallery\n",
        "                    if id_saved_count[pid] == 0:\n",
        "                        save_dir = os.path.join(OUTPUT_ROOT, 'test', 'query', pid_str)\n",
        "                    elif id_saved_count[pid] % gallery_freq == 0:\n",
        "                        save_dir = os.path.join(OUTPUT_ROOT, 'test', 'gallery', pid_str)\n",
        "                    else:\n",
        "                        id_saved_count[pid] += 1\n",
        "                        continue\n",
        "\n",
        "                os.makedirs(save_dir, exist_ok=True)\n",
        "                cv2.imwrite(os.path.join(save_dir, save_name), crop)\n",
        "                id_saved_count[pid] += 1\n",
        "\n",
        "# Esecuzione\n",
        "if os.path.exists(OUTPUT_ROOT): shutil.rmtree(OUTPUT_ROOT)\n",
        "extract_crops(TRAIN_SEQUENCES, split_type='train')\n",
        "extract_crops(TEST_SEQUENCES, split_type='test', gallery_freq=10)\n",
        "print(f\"\\n‚úÖ Generazione completata! Identit√† totali create: {next_global_pid}\")"
      ],
      "metadata": {
        "id": "9y_3WBmTJlNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 2. SMART DATASET BALANCING & MIGRATION\n",
        "# ==========================================\n",
        "import os\n",
        "import shutil\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "TRAIN_DIR = './soccernet_reid_robust/train'\n",
        "QUERY_DIR = './soccernet_reid_robust/test/query'\n",
        "GALLERY_DIR = './soccernet_reid_robust/test/gallery'\n",
        "\n",
        "def balance_dataset_smart(root_dir, keep_freq=5, min_threshold=50):\n",
        "    \"\"\"Decima le immagini delle classi sovrarappresentate salvaguardando la coda lunga.\"\"\"\n",
        "    print(f\"\\nüöÄ Inizio Bilanciamento in: {root_dir} (Keep 1 every {keep_freq} if total > {min_threshold})\")\n",
        "    stats = {\"before\": 0, \"deleted\": 0, \"preserved_classes\": 0}\n",
        "\n",
        "    for pid in tqdm(sorted(os.listdir(root_dir)), desc=\"Balancing Classes\"):\n",
        "        pid_path = os.path.join(root_dir, pid)\n",
        "        if not os.path.isdir(pid_path): continue\n",
        "\n",
        "        imgs = sorted([f for f in os.listdir(pid_path) if f.endswith('.jpg')])\n",
        "        stats[\"before\"] += len(imgs)\n",
        "\n",
        "        if len(imgs) <= min_threshold:\n",
        "            stats[\"preserved_classes\"] += 1\n",
        "            continue\n",
        "\n",
        "        for i, img_name in enumerate(imgs):\n",
        "            if i % keep_freq != 0:\n",
        "                os.remove(os.path.join(pid_path, img_name))\n",
        "                stats[\"deleted\"] += 1\n",
        "\n",
        "    print(f\"‚úÖ Bilanciamento completato! Rimosse {stats['deleted']} immagini superflue. \"\n",
        "          f\"Classi povere salvaguardate: {stats['preserved_classes']}\")\n",
        "\n",
        "def migrate_test_to_train(num_classes_to_move=50):\n",
        "    \"\"\"Sposta classi dal Test al Train per aumentare la diversit√† del training set.\"\"\"\n",
        "    gallery_classes = sorted(os.listdir(GALLERY_DIR))\n",
        "    if len(gallery_classes) < num_classes_to_move: return\n",
        "\n",
        "    last_train_id = int(sorted(os.listdir(TRAIN_DIR))[-1]) if os.listdir(TRAIN_DIR) else -1\n",
        "    next_id = last_train_id + 1\n",
        "\n",
        "    print(f\"\\nüöÄ Migrazione di {num_classes_to_move} classi da Test a Train...\")\n",
        "    for old_class in tqdm(gallery_classes[:num_classes_to_move], desc=\"Migrating\"):\n",
        "        new_class_name = f\"{next_id:05d}\"\n",
        "        dst_path = os.path.join(TRAIN_DIR, new_class_name)\n",
        "        os.makedirs(dst_path, exist_ok=True)\n",
        "\n",
        "        for src_dir in [QUERY_DIR, GALLERY_DIR]:\n",
        "            src_class_path = os.path.join(src_dir, old_class)\n",
        "            if os.path.exists(src_class_path):\n",
        "                for img in os.listdir(src_class_path):\n",
        "                    shutil.move(os.path.join(src_class_path, img), os.path.join(dst_path, img))\n",
        "                os.rmdir(src_class_path)\n",
        "        next_id += 1\n",
        "    print(f\"‚úÖ Migrazione completata! Nuovo range Train ID: fino a {next_id-1:05d}\")\n",
        "\n",
        "# Esecuzione Pipeline di Ottimizzazione\n",
        "migrate_test_to_train(num_classes_to_move=30)\n",
        "balance_dataset_smart(TRAIN_DIR, keep_freq=5, min_threshold=50)"
      ],
      "metadata": {
        "id": "f8bOmfuLJm1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 3. DATASET EXPLORATORY DATA ANALYSIS (EDA)\n",
        "# ==========================================\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "subsets = {\n",
        "    'TRAIN': './soccernet_reid_robust/train',\n",
        "    'QUERY': './soccernet_reid_robust/test/query',\n",
        "    'GALLERY': './soccernet_reid_robust/test/gallery'\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(18, 5))\n",
        "\n",
        "for i, (name, path) in enumerate(subsets.items()):\n",
        "    if not os.path.exists(path): continue\n",
        "\n",
        "    pids = [p for p in sorted(os.listdir(path)) if os.path.isdir(os.path.join(path, p))]\n",
        "    counts = np.array([len(os.listdir(os.path.join(path, pid))) for pid in pids])\n",
        "\n",
        "    if len(counts) == 0: continue\n",
        "\n",
        "    print(f\"\\nüìä {name} SET -> Classi: {len(pids)} | Img Totali: {np.sum(counts)} | \"\n",
        "          f\"Media/Classe: {np.mean(counts):.1f} | Max Img: {np.max(counts)}\")\n",
        "\n",
        "    plt.subplot(1, 3, i+1)\n",
        "    plt.hist(counts, bins=30, color='#4CAF50' if name=='TRAIN' else '#2196F3', edgecolor='black', alpha=0.7)\n",
        "    plt.title(f'Distribuzione {name}\\n(Tot: {len(counts)} classi)')\n",
        "    plt.xlabel('Numero di Immagini per Classe')\n",
        "    plt.ylabel('Frequenza (N. Classi)')\n",
        "    plt.axvline(np.mean(counts), color='red', linestyle='dashed', linewidth=2, label=f'Media: {np.mean(counts):.1f}')\n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "    plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PpDHyAHEJuCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsRKKNjNg-Tl"
      },
      "source": [
        "# Re-Identification\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 1. CUSTOM AUGMENTATIONS & TRANSFORMS\n",
        "# ==========================================\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "import random\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "class RandomMotionBlur:\n",
        "    \"\"\"\n",
        "    Applica un Motion Blur casuale (effetto scia).\n",
        "    Simula movimenti rapidi dei giocatori o pan/tilt veloci della telecamera.\n",
        "    \"\"\"\n",
        "    def __init__(self, p=0.5, kernel_size=(3, 10), angle_range=(-45, 45)):\n",
        "        self.p = p\n",
        "        self.kernel_size = kernel_size\n",
        "        self.angle_range = angle_range\n",
        "\n",
        "    def __call__(self, img):\n",
        "        if random.random() > self.p: return img\n",
        "\n",
        "        image = np.array(img)\n",
        "        k = random.randint(self.kernel_size[0], self.kernel_size[1])\n",
        "        if k % 2 == 0: k += 1\n",
        "\n",
        "        # Generazione Kernel direzionale\n",
        "        kernel = np.zeros((k, k))\n",
        "        kernel[int((k-1)/2), :] = np.ones(k)\n",
        "        kernel /= k\n",
        "\n",
        "        # Rotazione casuale per la direzione del movimento\n",
        "        angle = random.randint(self.angle_range[0], self.angle_range[1])\n",
        "        if angle != 0:\n",
        "            M = cv2.getRotationMatrix2D((k/2, k/2), angle, 1)\n",
        "            kernel = cv2.warpAffine(kernel, M, (k, k))\n",
        "\n",
        "        blurred = cv2.filter2D(image, -1, kernel)\n",
        "        return Image.fromarray(blurred)\n",
        "\n",
        "def build_custom_transforms(height=256, width=128, is_train=True):\n",
        "    \"\"\"Costruisce la pipeline di trasformazioni (PIL -> Tensor -> Normalization).\"\"\"\n",
        "    if is_train:\n",
        "        heavy_transforms_pil = T.Compose([\n",
        "            RandomMotionBlur(p=0.5, kernel_size=(3, 9), angle_range=(-15, 15)),\n",
        "            T.RandomAffine(degrees=0, translate=(0.05, 0.05), scale=(0.8, 1.2), fill=(85, 115, 85)),\n",
        "            T.ColorJitter(brightness=0.2, contrast=0.15, saturation=0.15, hue=0.1),\n",
        "            T.RandomApply([T.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5))], p=0.3),\n",
        "        ])\n",
        "\n",
        "        return T.Compose([\n",
        "            T.Resize((height, width)),\n",
        "            T.Pad(10, padding_mode='edge'),\n",
        "            T.RandomCrop((height, width)),\n",
        "            T.RandomHorizontalFlip(p=0.5),\n",
        "            T.RandomApply([heavy_transforms_pil], p=0.7),\n",
        "            T.ToTensor(),\n",
        "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            T.RandomErasing(p=0.5, scale=(0.02, 0.40), ratio=(0.3, 3.3), value=0) # Sul tensore!\n",
        "        ])\n",
        "    else:\n",
        "        return T.Compose([\n",
        "            T.Resize((height, width)),\n",
        "            T.ToTensor(),\n",
        "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "\n",
        "print(\"‚úÖ Trasformazioni Custom Inizializzate!\")"
      ],
      "metadata": {
        "id": "C47EM_0CLwVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 2. SOCCER-NET RE-ID DATASET CLASS\n",
        "# ==========================================\n",
        "import os\n",
        "import torchreid\n",
        "from torchreid.data import ImageDataset\n",
        "\n",
        "class SoccerNetReID(ImageDataset):\n",
        "    \"\"\"\n",
        "    Dataset Adapter per caricare i dati strutturati in Torchreid.\n",
        "    \"\"\"\n",
        "    def __init__(self, root='', **kwargs):\n",
        "        self.root = root\n",
        "        self.train_dir = os.path.join(self.root, 'train')\n",
        "        self.query_dir = os.path.join(self.root, 'test', 'query')\n",
        "        self.gallery_dir = os.path.join(self.root, 'test', 'gallery')\n",
        "\n",
        "        train = self.process_dir(self.train_dir, is_query=False)\n",
        "        query = self.process_dir(self.query_dir, is_query=True)\n",
        "        gallery = self.process_dir(self.gallery_dir, is_query=False)\n",
        "\n",
        "        super().__init__(train, query, gallery, **kwargs)\n",
        "\n",
        "    def process_dir(self, dir_path, is_query=False):\n",
        "        data = []\n",
        "        if not os.path.exists(dir_path): return data\n",
        "\n",
        "        # Ordine alfabetico per garantire che gli ID (0, 1, 2...) siano sempre coerenti\n",
        "        pid_dirs = sorted([p for p in os.listdir(dir_path) if os.path.isdir(os.path.join(dir_path, p))])\n",
        "\n",
        "        for pid_idx, pid_folder in enumerate(pid_dirs):\n",
        "            pid_path = os.path.join(dir_path, pid_folder)\n",
        "            imgs = [f for f in os.listdir(pid_path) if f.endswith(('.png', '.jpg'))]\n",
        "\n",
        "            if len(imgs) < 1: continue\n",
        "\n",
        "            for img_name in imgs:\n",
        "                img_path = os.path.join(pid_path, img_name)\n",
        "                camid = 1 if is_query else 0 # Torchreid richiede ID camera fittizi se non noti\n",
        "                data.append((img_path, pid_idx, camid))\n",
        "\n",
        "        return data\n",
        "\n",
        "# --- REGISTRAZIONE SICURA IN TORCHREID ---\n",
        "dataset_name = 'soccernet-reid'\n",
        "if dataset_name in torchreid.data.datasets.__image_datasets:\n",
        "    del torchreid.data.datasets.__image_datasets[dataset_name]\n",
        "\n",
        "torchreid.data.register_image_dataset(dataset_name, SoccerNetReID)\n",
        "print(f\"‚úÖ Dataset '{dataset_name}' registrato con successo nel framework Torchreid!\")"
      ],
      "metadata": {
        "id": "oz_qZVDiLz-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 3. DATAMANAGER & PK-SAMPLER VISUALIZATION\n",
        "# ==========================================\n",
        "from torchreid.data import ImageDataManager\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "\n",
        "print(\"üöÄ Inizializzazione DataManager (con RandomIdentitySampler per Triplet Loss)...\")\n",
        "\n",
        "# Setup DataManager\n",
        "datamanager = ImageDataManager(\n",
        "    root='./dataset_reid_clean',   # Punta direttamente ai dati puliti dello step precedente!\n",
        "    sources='soccernet-reid',\n",
        "    targets='soccernet-reid',\n",
        "    height=256,\n",
        "    width=128,\n",
        "    batch_size_train=64,\n",
        "    batch_size_test=100,\n",
        "    transforms=['random_flip'],    # Override necessario interno, ma usiamo la nostra pipeline\n",
        "    train_sampler='RandomIdentitySampler', # FONDAMENTALE per la Triplet Loss\n",
        "    num_instances=4,               # K = 4 immagini per ogni P (persona)\n",
        "    workers=2\n",
        ")\n",
        "\n",
        "# Applichiamo le nostre trasformazioni avanzate\n",
        "datamanager.train_loader.dataset.transform = build_custom_transforms(is_train=True)\n",
        "datamanager.test_loader['soccernet-reid']['query'].dataset.transform = build_custom_transforms(is_train=False)\n",
        "datamanager.test_loader['soccernet-reid']['gallery'].dataset.transform = build_custom_transforms(is_train=False)\n",
        "\n",
        "print(f\"‚úÖ DataLoader pronto. Immagini di training caricate: {len(datamanager.train_loader.dataset)}\")\n",
        "\n",
        "# --- VISUALIZZAZIONE DEL BATCH ---\n",
        "def denormalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
        "    for t, m, s in zip(tensor, mean, std): t.mul_(s).add_(m)\n",
        "    return tensor\n",
        "\n",
        "def visualize_pk_batch(dataloader, num_instances=4):\n",
        "    \"\"\"Estrazione e visualizzazione di un batch per validare il PxK Sampler.\"\"\"\n",
        "    batch = next(iter(dataloader))\n",
        "    imgs, pids = batch['img'] if isinstance(batch, dict) else batch[0], batch['pid'] if isinstance(batch, dict) else batch[1]\n",
        "\n",
        "    unique_pids, counts = torch.unique(pids, return_counts=True)\n",
        "\n",
        "    print(\"\\nüïµÔ∏è Analisi del Sampler (PK-Batch Check):\")\n",
        "    print(f\"   ID unici nel batch: {len(unique_pids)}\")\n",
        "    print(f\"   Shape Immagini: {imgs.shape}\")\n",
        "\n",
        "    if all(c == num_instances for c in counts):\n",
        "        print(f\"   ‚úÖ SUCCESS: Ogni ID ha esattamente {num_instances} istanze. Triplet Loss ottimizzata!\")\n",
        "    else:\n",
        "        print(f\"   ‚ö†Ô∏è WARNING: Il sampler non rispetta strettamente K={num_instances}.\")\n",
        "\n",
        "    # Plot (Primi 4 ID)\n",
        "    grouped_images = defaultdict(list)\n",
        "    for i, pid in enumerate(pids): grouped_images[pid.item()].append(imgs[i])\n",
        "    selected_pids = list(grouped_images.keys())[:4]\n",
        "\n",
        "    if not selected_pids: return\n",
        "\n",
        "    fig, axes = plt.subplots(len(selected_pids), num_instances, figsize=(12, 3 * len(selected_pids)))\n",
        "    if len(selected_pids) == 1: axes = np.expand_dims(axes, axis=0)\n",
        "\n",
        "    for row_idx, pid in enumerate(selected_pids):\n",
        "        for col_idx in range(num_instances):\n",
        "            ax = axes[row_idx, col_idx]\n",
        "            if col_idx < len(grouped_images[pid]):\n",
        "                img_t = denormalize(grouped_images[pid][col_idx].clone())\n",
        "                ax.imshow(np.clip(img_t.permute(1, 2, 0).cpu().numpy(), 0, 1))\n",
        "                ax.set_title(f\"ID: {pid}\")\n",
        "            else:\n",
        "                ax.text(0.5, 0.5, 'Missing', ha='center')\n",
        "            ax.axis('off')\n",
        "\n",
        "    plt.suptitle(\"Validazione Batch: P identit√† x K istanze\", fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Esegui l'analisi\n",
        "visualize_pk_batch(datamanager.train_loader)"
      ],
      "metadata": {
        "id": "QMyIlIT2L3Ee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95a-eMKW_N2e"
      },
      "source": [
        "# Avvio training Architettura Ibrida CNN-Transformer\n",
        "OSNet + CBAM + MHSA + GeM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWnOLsPz_N2f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchreid\n",
        "from torchreid import models, metrics\n",
        "from torchreid.losses import TripletLoss, CrossEntropyLoss\n",
        "\n",
        "import os\n",
        "import time\n",
        "import datetime\n",
        "import numpy as np\n",
        "import shutil\n",
        "\n",
        "\n",
        "# riporto la classe per mantenere early stopping, override dei metodi save e run.\n",
        "class EarlyStoppingTripletEngine(torchreid.engine.ImageTripletEngine):\n",
        "\n",
        "    def save_model(self, epoch, rank1, map_score, save_dir, is_best=False):\n",
        "        \"\"\"Override: Salva solo Last (sovrascritto) e Best.\"\"\"\n",
        "        state = {\n",
        "            'state_dict': self.model.state_dict(),\n",
        "            'epoch': epoch + 1,\n",
        "            'rank1': rank1,\n",
        "            'map': map_score,  # Ora possiamo salvarlo correttamente\n",
        "            'optimizer': self.optimizer.state_dict(),\n",
        "            'scheduler': self.scheduler.state_dict() if self.scheduler else None\n",
        "        }\n",
        "\n",
        "        # 1. Salva/Sovrascrive sempre model_last.pth.tar\n",
        "        last_fpath = os.path.join(save_dir, 'model/model_last.pth.tar')\n",
        "        torch.save(state, last_fpath)\n",
        "\n",
        "        # 2. Se √® best, crea una copia chiamata model_best.pth.tar\n",
        "        if is_best:\n",
        "            best_fpath = os.path.join(save_dir, 'model/model_best.pth.tar')\n",
        "            shutil.copy(last_fpath, best_fpath)\n",
        "\n",
        "    def test(self, dist_metric='euclidean', normalize_feature=True,\n",
        "             visrank=False, visrank_topk=10, save_dir='',\n",
        "             use_metric_cuhk03=False, ranks=[1, 5, 10, 20], rerank=False):\n",
        "        \"\"\"\n",
        "        Override semplificato: usa la logica interna _evaluate ma ritorna anche mAP.\n",
        "        \"\"\"\n",
        "        self.set_model_mode('eval')\n",
        "        targets = list(self.test_loader.keys())\n",
        "\n",
        "        final_rank1 = 0\n",
        "        final_mAP = 0\n",
        "\n",
        "        for name in targets:\n",
        "            domain = 'source' if name in self.datamanager.sources else 'target'\n",
        "            print('##### Evaluating {} ({}) #####'.format(name, domain))\n",
        "\n",
        "            # Recuperiamo i loader specifici\n",
        "            query_loader = self.test_loader[name]['query']\n",
        "            gallery_loader = self.test_loader[name]['gallery']\n",
        "\n",
        "            # USIAMO IL METODO NATIVO DEL PADRE\n",
        "            # _evaluate fa gi√†: extract_features -> compute_distmat -> evaluate_rank\n",
        "            rank1, mAP = self._evaluate(\n",
        "                dataset_name=name,\n",
        "                query_loader=query_loader,\n",
        "                gallery_loader=gallery_loader,\n",
        "                dist_metric=dist_metric,\n",
        "                normalize_feature=normalize_feature,\n",
        "                visrank=visrank,\n",
        "                visrank_topk=visrank_topk,\n",
        "                save_dir=save_dir,\n",
        "                use_metric_cuhk03=use_metric_cuhk03,\n",
        "                ranks=ranks,\n",
        "                rerank=rerank\n",
        "            )\n",
        "\n",
        "            # Logghiamo su Tensorboard anche qui per sicurezza\n",
        "            if self.writer is not None:\n",
        "                self.writer.add_scalar(f'Test/{name}/rank1', rank1, self.epoch)\n",
        "                self.writer.add_scalar(f'Test/{name}/mAP', mAP, self.epoch)\n",
        "\n",
        "            # Aggiorniamo i valori finali (nel caso multi-dataset prende l'ultimo)\n",
        "            final_rank1 = rank1\n",
        "            final_mAP = mAP\n",
        "\n",
        "        # QUESTA √à LA MODIFICA CHIAVE: Ritorniamo la tupla\n",
        "        return final_rank1, final_mAP\n",
        "\n",
        "\n",
        "    def run(self, save_dir='log', max_epoch=60, start_epoch=0,\n",
        "            print_freq=10, fixbase_epoch=0, open_layers=None,\n",
        "            start_eval=0, eval_freq=-1, test_only=False,\n",
        "            dist_metric='euclidean', normalize_feature=True,\n",
        "            visrank=False, visrank_topk=10, use_metric_cuhk03=False,\n",
        "            ranks=[1, 5, 10, 20], rerank=False,\n",
        "            patience=10): # <--- Parametro aggiunto\n",
        "\n",
        "        # Setup iniziale (copiato dalla logica base)\n",
        "        if test_only:\n",
        "            self.test(dist_metric, normalize_feature, visrank, visrank_topk,\n",
        "                      save_dir, use_metric_cuhk03, ranks, rerank)\n",
        "            return\n",
        "\n",
        "        if self.writer is None:\n",
        "            from torch.utils.tensorboard import SummaryWriter\n",
        "            self.writer = SummaryWriter(log_dir=save_dir)\n",
        "\n",
        "        time_start = time.time()\n",
        "        self.start_epoch = start_epoch\n",
        "        self.max_epoch = max_epoch\n",
        "        print(f'=> Start training with Early Stopping (Patience: {patience})')\n",
        "\n",
        "        # Variabili per Early Stopping\n",
        "        best_rank1 = -np.inf\n",
        "        patience_counter = 0\n",
        "\n",
        "        for self.epoch in range(self.start_epoch, self.max_epoch):\n",
        "            # 1. Fase di Training (usa il metodo della classe padre)\n",
        "            self.train(print_freq=print_freq, fixbase_epoch=fixbase_epoch, open_layers=open_layers)\n",
        "\n",
        "            # 2. Valutazione (se siamo nell'epoca giusta o se eval_freq √® settato)\n",
        "            # Nota: eval_freq=-1 significa \"valuta solo alla fine\", qui forziamo\n",
        "            # una valutazione se vogliamo usare l'early stopping in modo sensato,\n",
        "            # tipicamente si valuta ogni epoca dopo start_eval.\n",
        "            should_eval = (self.epoch + 1) >= start_eval and \\\n",
        "                          (eval_freq > 0 and (self.epoch+1) % eval_freq == 0)\n",
        "\n",
        "            if should_eval:\n",
        "                print(f\"üîç Validazione Epoca {self.epoch + 1}...\")\n",
        "\n",
        "                # Esegue il test (usa il metodo della classe padre)\n",
        "                rank1, mAP = self.test(\n",
        "                    dist_metric=dist_metric,\n",
        "                    normalize_feature=normalize_feature,\n",
        "                    visrank=visrank,\n",
        "                    visrank_topk=visrank_topk,\n",
        "                    save_dir=save_dir,\n",
        "                    use_metric_cuhk03=use_metric_cuhk03,\n",
        "                    ranks=ranks\n",
        "                )\n",
        "\n",
        "                # Log su Tensorboard\n",
        "                if self.writer is not None:\n",
        "                    self.writer.add_scalar('Val/Rank1', rank1, self.epoch)\n",
        "\n",
        "                # 3. Logica Early Stopping & Salvataggio\n",
        "                is_best = False\n",
        "\n",
        "                # Caso A: Miglior Rank-1\n",
        "                if rank1 > best_rank1:\n",
        "                    print(f\"‚≠ê NUOVO BEST Rank-1: {rank1:.1%} (prev: {best_rank1:.1%}) | mAP: {mAP:.1%}\")\n",
        "                    best_rank1 = rank1\n",
        "                    best_map = mAP\n",
        "                    is_best = True\n",
        "\n",
        "                # Caso B: Stesso Rank-1, Miglior mAP\n",
        "                elif rank1 == best_rank1 and mAP > best_map:\n",
        "                    print(f\"‚≠ê Rank-1 Invariato ({rank1:.1%}), ma mAP MIGLIORATO: {mAP:.1%} > {best_map:.1%}\")\n",
        "                    best_map = mAP\n",
        "                    is_best = True\n",
        "\n",
        "                if is_best:\n",
        "                    patience_counter = 0\n",
        "                else:\n",
        "                    patience_counter += 1\n",
        "                    print(f\"‚è≥ Nessun miglioramento. Patience: {patience_counter}/{patience}. Best R1: {best_rank1:.1%} (mAP {best_map:.1%})\")\n",
        "\n",
        "                self.save_model(self.epoch, rank1, mAP, save_dir, is_best=is_best)\n",
        "\n",
        "                if patience_counter >= patience:\n",
        "                    print(f\"üõë EARLY STOPPING attivato all'epoca {self.epoch + 1}\")\n",
        "                    break\n",
        "            else:\n",
        "                self.save_model(self.epoch, 0, 0, save_dir, is_best=False)\n",
        "\n",
        "        # Chiusura\n",
        "        elapsed = round(time.time() - time_start)\n",
        "        elapsed = str(datetime.timedelta(seconds=elapsed))\n",
        "        print('Elapsed {}'.format(elapsed))\n",
        "        if self.writer is not None:\n",
        "            self.writer.close()\n",
        "\n",
        "\n",
        "class VisionTransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, heads=4, height=16, width=8, dropout=0.1, debug_freq=20):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.heads = heads\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "\n",
        "        # --- AGGIUNTA PER DEBUG ---\n",
        "        self.debug_freq = debug_freq  # Ogni quanti batch stampare\n",
        "        self.batch_counter = 0        # Contatore interno\n",
        "        # --------------------------\n",
        "\n",
        "        # Proiezioni Q, K, V usando Linear (come il prof)\n",
        "        # Nota: Linear richiede input (Batch, Seq_Len, Dim)\n",
        "        self.query_projection = nn.Linear(dim, dim)\n",
        "        self.key_projection = nn.Linear(dim, dim)\n",
        "        self.value_projection = nn.Linear(dim, dim)\n",
        "        self.output_projection = nn.Linear(dim, dim)\n",
        "\n",
        "        # Positional Embedding IMPARABILE (stile ViT)\n",
        "        # Invece di quello sinusoidale del prof (ottimo per testo),\n",
        "        # per le immagini fisse conviene lasciare che la rete impari la posizione.\n",
        "        self.num_patches = height * width\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches, dim))\n",
        "\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Input x: (Batch, Channel, Height, Width) -> Es. (B, 512, 16, 8)\n",
        "        \"\"\"\n",
        "\n",
        "        # --- LOGICA DI DEBUG PERIODICA ---\n",
        "        if self.training:\n",
        "            self.batch_counter += 1\n",
        "            # Stampa solo se il contatore √® multiplo della frequenza scelta\n",
        "            if self.batch_counter % self.debug_freq == 0:\n",
        "                print(f\"üîç [ViT Heartbeat] Batch {self.batch_counter} | Input Shape: {x.shape} (Atteso H={self.height}, W={self.width})\")\n",
        "        # ---------------------------------\n",
        "\n",
        "        B, C, H, W = x.shape\n",
        "\n",
        "        # 1. FLATTEN: Trasformiamo l'immagine 2D in una sequenza 1D\n",
        "        # Da (B, C, H, W) -> (B, H*W, C)\n",
        "        # .permute(0, 2, 3, 1) mette i canali alla fine\n",
        "        # .flatten(1, 2) schiaccia H e W insieme\n",
        "        x_flat = x.permute(0, 2, 3, 1).flatten(1, 2) # (B, SeqLen, Dim)\n",
        "\n",
        "        # 2. Aggiunta Positional Embedding\n",
        "        # Se le dimensioni cambiano dinamicamente, interpoliamo il pos embedding\n",
        "        if x_flat.shape[1] != self.pos_embedding.shape[1]:\n",
        "             # Gestione sicurezza ridimensionamento\n",
        "             pos_emb = F.interpolate(\n",
        "                 self.pos_embedding.permute(0, 2, 1).view(1, C, self.height, self.width),\n",
        "                 size=(H, W), mode='bilinear').flatten(2).permute(0, 2, 1)\n",
        "             x_flat = x_flat + pos_emb\n",
        "        else:\n",
        "             x_flat = x_flat + self.pos_embedding\n",
        "\n",
        "        # 3. Layer Norm prima dell'attention (Pre-Norm architecture, pi√π stabile)\n",
        "        residual = x_flat\n",
        "        x_norm = self.norm(x_flat)\n",
        "\n",
        "        # 4. Proiezioni Lineari\n",
        "        Q = self.query_projection(x_norm)\n",
        "        K = self.key_projection(x_norm)\n",
        "        V = self.value_projection(x_norm)\n",
        "\n",
        "        # 5. Split Heads (come nel codice del prof, ma ottimizzato con view)\n",
        "        # (B, SeqLen, Heads, HeadDim) -> (B, Heads, SeqLen, HeadDim)\n",
        "        Q = Q.view(B, -1, self.heads, C // self.heads).transpose(1, 2)\n",
        "        K = K.view(B, -1, self.heads, C // self.heads).transpose(1, 2)\n",
        "        V = V.view(B, -1, self.heads, C // self.heads).transpose(1, 2)\n",
        "\n",
        "        # 6. SCALED DOT PRODUCT ATTENTION (Il cuore ottimizzato)\n",
        "        # is_causal=False perch√© nell'immagine guardiamo tutto il contesto\n",
        "        out = F.scaled_dot_product_attention(Q, K, V, dropout_p=0.1 if self.training else 0.0, is_causal=False)\n",
        "\n",
        "        # 7. Join Heads\n",
        "        out = out.transpose(1, 2).contiguous().view(B, -1, C)\n",
        "\n",
        "        # 8. Output Projection + Residual\n",
        "        out = self.output_projection(out)\n",
        "        out = out + residual\n",
        "\n",
        "        # 9. UNFLATTEN: Torniamo a immagine 2D\n",
        "        # Da (B, SeqLen, C) -> (B, C, H, W)\n",
        "        out = out.permute(0, 2, 1).view(B, C, H, W)\n",
        "\n",
        "        return out\n",
        "\n",
        "# --- 1. Moduli Base (CBAM, GeM) restano uguali ---\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, in_planes, ratio=16):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "\n",
        "        # 1. Average Pooling\n",
        "        # Riduce (H, W) a (1, 1) facendo la media.\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        # 2. Max Pooling\n",
        "        # Riduce (H, W) a (1, 1) prendendo il valore massimo.\n",
        "        # Serve a preservare le feature di texture pi√π forti.\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "\n",
        "        # 3. COMPRESSIONE (Shared MLP)\n",
        "        # Questo √® il primo strato del \"bottleneck\" che riduce i canali.\n",
        "        # Usa Conv2d con kernel 1x1 che equivale a un Fully Connected layer.\n",
        "        self.fc1 = nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False)\n",
        "        self.relu1 = nn.ReLU()\n",
        "\n",
        "        # 4. RIESPANSIONE (Shared MLP)\n",
        "        # Ripristina il numero originale di canali.\n",
        "        self.fc2 = nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n",
        "\n",
        "        # 5. GATING (Sigmoide)\n",
        "        # Prepara l'attivazione tra 0 e 1 per pesare i canali.\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Le stesse fc1 e fc2 vengono usate SIA per il vettore delle medie SIA per quello dei massimi.\n",
        "\n",
        "        # Percorso A: Quello che descrivevi tu (Avg -> MLP)\n",
        "\n",
        "        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n",
        "\n",
        "        # Percorso B: L'aggiunta del CBAM (Max -> MLP)\n",
        "        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n",
        "\n",
        "        # a questo punto il tensore in ingresso √® stato trasformato in due vettori monodimensionali, lunghi la dimensione dei canali.\n",
        "\n",
        "        # FUSIONE\n",
        "        # I due vettori vengono sommati element-wise.\n",
        "        out = avg_out + max_out\n",
        "\n",
        "        # ATTENZIONE:\n",
        "        # Questo blocco restituisce SOLO i pesi (la maschera di attenzione),\n",
        "        # NON esegue la moltiplicazione finale per l'input 'x'.\n",
        "\n",
        "        return self.sigmoid(out) #restituisce vettore con valori di attention per ogni feature map del tensore x.\n",
        "\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super(SpatialAttention, self).__init__()\n",
        "        # Per mantenere le dimensioni H x W inalterate dopo una conv 7x7,\n",
        "        # serve un padding di 3. (formula: p = (k-1)/2)\n",
        "        padding = 3 if kernel_size == 7 else 1\n",
        "\n",
        "        # Questa convoluzione ridurr√† i 2 canali di input (Max+Avg) a 1 canale di output.\n",
        "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x √® il tuo tensore di input [Batch, Canali, Altezza, Larghezza]\n",
        "\n",
        "        # 1. COMPRESSIONE DEI CANALI (Average Pooling)\n",
        "        # Invece di fare la media spaziale (come nel blocco Channel), qui facciamo la media SUI CANALI.\n",
        "        # \"In media, quanto √® attiva questa posizione (h,w) su tutti i filtri?\"\n",
        "        # Output: [B, 1, H, W]\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
        "\n",
        "        # 2. COMPRESSIONE DEI CANALI (Max Pooling)\n",
        "        # \"Qual √® la feature pi√π forte in assoluto in questa posizione?\"\n",
        "        # Questo √® utilissimo per trovare i bordi o dettagli unici del giocatore.\n",
        "        # Output: [B, 1, H, W]\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "\n",
        "        # 3. CONCATENAZIONE\n",
        "        # Uniamo le due mappe. Ora abbiamo una rappresentazione \"grezza\" di dove si trovano le informazioni.\n",
        "        # Output: [B, 2, H, W]\n",
        "        x = torch.cat([avg_out, max_out], dim=1)\n",
        "\n",
        "        # 4. CONVOLUZIONE SPAZIALE\n",
        "        # Qui sta l'intelligenza locale. Un kernel grande (7x7) scorre sull'immagine.\n",
        "        # Impara a capire che se c'√® un picco di attivazione in un punto,\n",
        "        # probabilmente √® importante anche l'area subito vicina.\n",
        "        # Trasforma i 2 canali in 1 solo canale di \"importanza spaziale\".\n",
        "        # Output: [B, 1, H, W]\n",
        "        x = self.conv1(x)\n",
        "\n",
        "        # 5. GENERAZIONE DELLA MASCHERA di dimensione [B, 1, H, W]\n",
        "        # Schiaccia i valori tra 0 e 1.\n",
        "        # 1 = \"Questa zona √® importante (il giocatore)\"\n",
        "        # 0 = \"Questa zona √® rumore (il campo)\"\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "class CBAM(nn.Module):\n",
        "    def __init__(self, in_planes, ratio=16, kernel_size=7):\n",
        "        super(CBAM, self).__init__()\n",
        "        self.ca = ChannelAttention(in_planes, ratio)\n",
        "        self.sa = SpatialAttention(kernel_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1. CHANNEL ATTENTION\n",
        "        out = x * self.ca(x) # Moltiplica l'input per i pesi dei canali\n",
        "\n",
        "        # 2. SPATIAL ATTENTION (applicata sull'output del channel)\n",
        "        result = out * self.sa(out) # Moltiplica il risultato raffinato per la maschera spaziale\n",
        "\n",
        "        return result\n",
        "\n",
        "class GeM(nn.Module):\n",
        "    def __init__(self, p=3.0, eps=1e-6):\n",
        "        super(GeM, self).__init__()\n",
        "\n",
        "        # 1. PARAMETRO P ADDESTRABILE\n",
        "        # Qui sta la magia. Non definiamo p come una costante (self.p = p),\n",
        "        # ma come un nn.Parameter.\n",
        "        # Questo dice a PyTorch: \"Durante la backpropagation, aggiorna anche questo valore\n",
        "        # per minimizzare la loss\".\n",
        "        # Inizializziamo a 3.0 perch√© √® un buon punto di partenza empirico per il ReID.\n",
        "        self.p = nn.Parameter(torch.ones(1) * p)\n",
        "\n",
        "        # 2. EPSILON\n",
        "        # Un valore piccolissimo per evitare divisioni per zero o radici di numeri instabili.\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: Tensore di input [Batch, Channels, Height, Width]\n",
        "\n",
        "        # 3. CLAMPING (Sicurezza Numerica)\n",
        "        # x.clamp(min=self.eps) forza tutti i valori nel tensore a essere almeno 1e-6.\n",
        "        # Evita NaN (Not a Number) durante il training.\n",
        "\n",
        "        # 4. ELEVAMENTO A POTENZA\n",
        "        # .pow(self.p) eleva ogni singolo pixel alla potenza p (che la rete sta imparando).\n",
        "        # Se p > 1, questo enfatizza i valori alti (i dettagli salienti del calciatore)\n",
        "        # e schiaccia verso zero i valori bassi (lo sfondo).\n",
        "\n",
        "        # 5. GLOBAL AVERAGE POOLING\n",
        "        # F.avg_pool2d(...) calcola la media spaziale.\n",
        "        # Il kernel size √® (x.size(-2), x.size(-1)), ovvero (Height, Width).\n",
        "        # Questo significa: \"Prendi tutta la feature map HxW e fanne una media unica\".\n",
        "        # Output parziale: [Batch, Channels, 1, 1]\n",
        "\n",
        "        # 6. RADICE P-ESIMA (Inverse Power)\n",
        "        # .pow(1. / self.p) applica la radice p-esima.\n",
        "        # Serve a riportare i valori alla scala originale (dopo averli elevati alla p prima della media).\n",
        "        return F.avg_pool2d(x.clamp(min=self.eps).pow(self.p), (x.size(-2), x.size(-1))).pow(1. / self.p)\n",
        "\n",
        "\n",
        "# --- 2. Modello HPM con Output Multipli ---\n",
        "class SoccerNetHybridModel(nn.Module):\n",
        "    def __init__(self, num_classes, model_name='osnet_x1_0', loss='triplet', best_weights_path=None):\n",
        "        super(SoccerNetHybridModel, self).__init__()\n",
        "        self.loss = loss\n",
        "\n",
        "        print(f\"üèóÔ∏è HYBRID Model 2.0: Backbone={model_name} + CBAM + VisionTransformer + GeM\")\n",
        "\n",
        "        # Caricamento del backbone OSNet ed eliminazione del classificatore.\n",
        "        # 1. Costruisci Backbone VUOTO (pretrained=False perch√© carichiamo i nostri)\n",
        "        base_model = models.build_model(name=model_name, num_classes=num_classes, pretrained=False, loss='triplet')\n",
        "\n",
        "        # --- CARICAMENTO PESI CUSTOM (WARM-UP) ---\n",
        "        if best_weights_path and os.path.exists(best_weights_path):\n",
        "            print(f\"‚ôªÔ∏è Caricamento pesi Backbone da: {best_weights_path}\")\n",
        "            checkpoint = torch.load(best_weights_path, weights_only=False)\n",
        "            state_dict = checkpoint['state_dict']\n",
        "\n",
        "            # Pulizia delle chiavi (Rimuove 'module.' e layer non backbone)\n",
        "            new_state_dict = {}\n",
        "            for k, v in state_dict.items():\n",
        "                k = k.replace(\"module.\", \"\") # Gestione DataParallel\n",
        "                # Carichiamo solo la parte feature extractor, ignoriamo classifier/fc vecchi\n",
        "                if not k.startswith(\"classifier\") and not k.startswith(\"fc\"):\n",
        "                    new_state_dict[k] = v\n",
        "\n",
        "            # Carica con strict=False per ignorare le chiavi mancanti (classifier)\n",
        "            base_model.load_state_dict(new_state_dict, strict=False)\n",
        "            print(\"‚úÖ Pesi Backbone caricati con successo!\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è Nessun peso custom fornito o file non trovato. Uso inizializzazione casuale.\")\n",
        "        # -----------------------------------------\n",
        "\n",
        "\n",
        "        self.backbone = base_model\n",
        "        if hasattr(self.backbone, 'classifier'): del self.backbone.classifier\n",
        "        if hasattr(self.backbone, 'fc'): del self.backbone.fc\n",
        "\n",
        "        # caricamento dei moduli CBAM e GeM\n",
        "        self.in_channels = 512\n",
        "        self.cbam = CBAM(self.in_channels)\n",
        "\n",
        "        # TRANSFORMER BLOCK (Attention Globale - Relazioni a lungo raggio)\n",
        "        # OSNet riduce le dimensioni di 16x.\n",
        "        # Se input standard ReID (256x128) -> Feature map (16x8)\n",
        "        self.trans = VisionTransformerBlock(self.in_channels, heads=4, height=16, width=8)\n",
        "\n",
        "        self.gem = GeM()\n",
        "\n",
        "        # 3. Head Singola\n",
        "        self.bn = nn.BatchNorm1d(self.in_channels)\n",
        "        self.bn.bias.requires_grad_(False)\n",
        "        self.bn.apply(self._weights_init_kaiming)\n",
        "\n",
        "        self.classifier = nn.Linear(self.in_channels, num_classes, bias=False)\n",
        "        self.classifier.apply(self._weights_init_classifier)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x, return_featuremaps=True)\n",
        "        # Output atteso: (B, 512, H, W)\n",
        "\n",
        "        features = self.cbam(features)\n",
        "        # CBAM preserva le dimensioni\n",
        "\n",
        "        # 3. TRANSFORMER (Collega le parti del corpo)\n",
        "        features = self.trans(features)\n",
        "\n",
        "        # Pooling (da H,W a 1,1) -> Flatten\n",
        "        global_features = self.gem(features).view(features.size(0), -1)\n",
        "\n",
        "        # Normalization\n",
        "        feat_norm = self.bn(global_features)\n",
        "\n",
        "        if self.training:\n",
        "            logits = self.classifier(feat_norm)\n",
        "            # Ritorna: (logits, features) per le due loss\n",
        "            if self.loss == 'triplet':\n",
        "                return logits, feat_norm\n",
        "            return logits\n",
        "        else:\n",
        "            # In inferenza: solo il vettore normalizzato\n",
        "            return feat_norm\n",
        "\n",
        "    def _weights_init_kaiming(self, m):\n",
        "        if isinstance(m, nn.BatchNorm1d):\n",
        "            nn.init.constant_(m.weight, 1.0); nn.init.constant_(m.bias, 0.0)\n",
        "    def _weights_init_classifier(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.normal_(m.weight, std=0.001);\n",
        "            if m.bias is not None: nn.init.constant_(m.bias, 0.0)\n",
        "\n",
        "\n",
        "# Configurazione\n",
        "model_name = 'osnet_x1_0'\n",
        "path_to_best_osnet = \"./runs/OSNet_1/model/model_best.pth.tar\"\n",
        "output_dir = './runs/HybridOSNet-CNN_Transformer'\n",
        "\n",
        "print(\"üöÄ Inizializzazione Deep Supervision Training...\")\n",
        "\n",
        "# --- CONFIGURAZIONE RIPRESA ADDESTRAMENTO ---\n",
        "RESUME = True  # Imposta a False per partire da zero\n",
        "model_path = os.path.join(output_dir, 'model/model_last.pth.tar')\n",
        "\n",
        "# 1. Istanzia Modello\n",
        "model = SoccerNetHybridModel(\n",
        "    num_classes=datamanager.num_train_pids,\n",
        "    model_name=model_name,\n",
        "    loss='triplet',\n",
        "    best_weights_path=path_to_best_osnet\n",
        ").cuda()\n",
        "\n",
        "# 2. Optimizer & Scheduler\n",
        "optimizer = torchreid.optim.build_optimizer(model, optim='adam', lr=0.0003)\n",
        "scheduler = torchreid.optim.build_lr_scheduler(optimizer, lr_scheduler='multi_step', stepsize=[25, 45])\n",
        "\n",
        "# --- LOGICA DI CARICAMENTO CHECKPOINT (RESUME) ---\n",
        "start_epoch = 0\n",
        "if RESUME and os.path.exists(model_path):\n",
        "    print(f\"üîÑ Ripresa addestramento dal checkpoint: {model_path}\")\n",
        "    checkpoint = torch.load(model_path, map_location='cuda' if torch.cuda.is_available() else 'cpu', weights_only=False)\n",
        "\n",
        "    # Carica i pesi del modello\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "    # Carica lo stato dell'ottimizzatore e dello scheduler\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    if scheduler and 'scheduler' in checkpoint and checkpoint['scheduler'] is not None:\n",
        "        scheduler.load_state_dict(checkpoint['scheduler'])\n",
        "\n",
        "    # Recupera l'epoca da cui ripartire\n",
        "    start_epoch = checkpoint['epoch']\n",
        "    print(f\"‚úÖ Checkpoint caricato. Riprendo dall'epoca {start_epoch}\")\n",
        "elif RESUME:\n",
        "    print(\"‚ö†Ô∏è Checkpoint non trovato, il training partir√† da zero.\")\n",
        "\n",
        "# 3. Istanzia NUOVO Engine\n",
        "engine = EarlyStoppingTripletEngine(\n",
        "    datamanager,\n",
        "    model,\n",
        "    optimizer,\n",
        "    margin=0.3,\n",
        "    weight_t=1.0,\n",
        "    weight_x=1.0,\n",
        "    scheduler=scheduler\n",
        ")\n",
        "\n",
        "my_open_layers = ['cbam', 'trans', 'gem', 'bn', 'classifier']\n",
        "\n",
        "# 4. Run\n",
        "engine.run(\n",
        "    save_dir=output_dir,\n",
        "    max_epoch=100,\n",
        "    start_epoch=start_epoch,\n",
        "    start_eval=25,\n",
        "    eval_freq=1,\n",
        "    patience=15,\n",
        "    fixbase_epoch=20 if start_epoch == 0 else 0,\n",
        "    open_layers=my_open_layers\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwLazcoH_N2i"
      },
      "source": [
        "Validazione"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80ssBIyW_N2j"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import torchreid\n",
        "\n",
        "# --- CONFIGURAZIONE ---\n",
        "# Scegli quale checkpoint testare: 'model_best.pth.tar' (il record) o 'model_last.pth.tar' (l'ultimo salvato)\n",
        "checkpoint_type = 'model_best.pth.tar'\n",
        "# checkpoint_type = 'model_last.pth.tar'\n",
        "\n",
        "output_dir = './runs/HybridOSNet-AIN'\n",
        "model_path = os.path.join(output_dir, 'model', checkpoint_type)\n",
        "model_name = 'osnet_ain_x1_0'\n",
        "\n",
        "print(f\"üîç Avvio Validazione Manuale su: {checkpoint_type}\")\n",
        "\n",
        "# 1. Istanziazione del Modello (Deve essere identica al Training per matchare i pesi)\n",
        "model = SoccerNetDeepSupervisionModel(\n",
        "    num_classes=datamanager.num_train_pids,\n",
        "    model_name=model_name,\n",
        "    num_stripes=4,\n",
        "    loss='triplet'\n",
        ").cuda()\n",
        "\n",
        "# 2. Imposta il modello in modalit√† Valutazione\n",
        "# (Disabilita Dropout e BatchNormalization in training mode)\n",
        "model.eval()\n",
        "\n",
        "# 3. Caricamento dei Pesi\n",
        "if os.path.exists(model_path):\n",
        "    print(f\"üì• Caricamento pesi da: {model_path}\")\n",
        "    # Nota: weights_only=False per evitare l'errore di sicurezza su file fidati\n",
        "    checkpoint = torch.load(model_path, map_location='cuda' if torch.cuda.is_available() else 'cpu', weights_only=False)\n",
        "\n",
        "    # Carica lo stato\n",
        "    try:\n",
        "        model.load_state_dict(checkpoint['state_dict'])\n",
        "        print(\"‚úÖ Pesi caricati con successo.\")\n",
        "    except RuntimeError as e:\n",
        "        print(f\"‚ùå Errore nel caricamento dei pesi (Mismatch architettura?): {e}\")\n",
        "else:\n",
        "    raise FileNotFoundError(f\"Il file {model_path} non esiste!\")\n",
        "\n",
        "# 4. Creazione dell'Optimizer Dummy\n",
        "# L'engine richiede un optimizer per essere inizializzato, anche se in test non serve.\n",
        "optimizer = torchreid.optim.build_optimizer(model, optim='adam', lr=0.0003)\n",
        "\n",
        "# 5. Istanziazione dell'Engine\n",
        "engine = MultiLossEngine(\n",
        "    datamanager,\n",
        "    model,\n",
        "    optimizer,\n",
        "    margin=0.3,\n",
        "    weight_t=1.0,\n",
        "    weight_x=1.0\n",
        ")\n",
        "\n",
        "# 6. Esecuzione del Test\n",
        "print(\"üöÄ Inizio calcolo metriche (CMC & mAP)...\")\n",
        "engine.run(\n",
        "    save_dir=output_dir,\n",
        "    test_only=True,  # <--- Questo dice all'engine di saltare il training e fare solo test\n",
        "    dist_metric='euclidean',\n",
        "    normalize_feature=True, # Importante per la distanza euclidea\n",
        "    visrank=False,          # Metti True se vuoi vedere le immagini dei risultati\n",
        "    visrank_topk=10\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tp4s6CDrDOUD"
      },
      "source": [
        "# Inizio Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CL9_vIvJ1sqC"
      },
      "source": [
        "\n",
        "dipendenze"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "hbfgL1b12Fad"
      },
      "outputs": [],
      "source": [
        "# Installa la versione corretta e completa da GitHub\n",
        "!pip install git+https://github.com/KaiyangZhou/deep-person-reid.git\n",
        "\n",
        "# Installa le dipendenze accessorie\n",
        "! pip install gdown\n",
        "\n",
        "! pip install ultralytics\n",
        "\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import importlib\n",
        "import re\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. PULIZIA E PREPARAZIONE (Reset dell'ambiente)\n",
        "# ==============================================================================\n",
        "REPO_PATH = '/content/yolo_tracking'\n",
        "\n",
        "print(\"üßπ 1. Pulizia vecchie installazioni...\")\n",
        "# Se la cartella esiste, la cancelliamo per scaricarla pulita (evita errori di file mancanti)\n",
        "if os.path.exists(REPO_PATH):\n",
        "    shutil.rmtree(REPO_PATH)\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. CLONAZIONE E INSTALLAZIONE\n",
        "# ==============================================================================\n",
        "print(\"üì• 2. Clonazione repository fresco...\")\n",
        "!git clone https://github.com/mikel-brostrom/yolo_tracking.git {REPO_PATH}\n",
        "\n",
        "print(\"üì¶ 3. Installazione dipendenze (requirements.txt)...\")\n",
        "# Questo installa lap, cython-bbox, etc.\n",
        "!pip install -q -r {REPO_PATH}/requirements.txt\n",
        "\n",
        "print(\"üîó 4. Installazione di boxmot in modalit√† editabile...\")\n",
        "# Questo collega la cartella a Python in modo permanente\n",
        "!pip install -q -e {REPO_PATH}\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. IL TUO SCRIPT DI IMPORTAZIONE (CORRETTO)\n",
        "# ==============================================================================\n",
        "print(\"\\nüîç 5. Avvio ricerca classe e importazione...\")\n",
        "\n",
        "# Percorso del file target\n",
        "target_file = os.path.join(REPO_PATH, 'boxmot', 'trackers', 'botsort', 'botsort.py')\n",
        "\n",
        "if os.path.exists(target_file):\n",
        "    print(f\"üìÇ File trovato: {target_file}\")\n",
        "\n",
        "    # Cerchiamo il nome della classe con una Regex\n",
        "    with open(target_file, 'r') as f:\n",
        "        content = f.read()\n",
        "        matches = re.findall(r'^class\\s+(\\w+)', content, re.MULTILINE)\n",
        "\n",
        "    if matches:\n",
        "        class_name = matches[0]\n",
        "        print(f\"‚úÖ Classe identificata nel codice: {class_name}\")\n",
        "\n",
        "        # Aggiungiamo il path per sicurezza\n",
        "        if REPO_PATH not in sys.path:\n",
        "            sys.path.append(REPO_PATH)\n",
        "\n",
        "        try:\n",
        "            # Importazione Dinamica\n",
        "            module_path = \"boxmot.trackers.botsort.botsort\"\n",
        "            module = importlib.import_module(module_path)\n",
        "\n",
        "            # Recuperiamo la classe dal modulo\n",
        "            BoTSORT_Class = getattr(module, class_name)\n",
        "\n",
        "            # Creiamo l'alias BoTSORT (come ti serve per il tuo codice)\n",
        "            BoTSORT = BoTSORT_Class\n",
        "\n",
        "            print(\"\\n\" + \"=\"*50)\n",
        "            print(f\"üöÄ {class_name} importata con successo!\")\n",
        "            print(f\"üìå Alias creato: 'BoTSORT' √® pronto all'uso.\")\n",
        "            print(\"=\"*50)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Errore durante l'importazione del modulo: {e}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Nessuna classe trovata nel file.\")\n",
        "else:\n",
        "    print(f\"‚ùå Errore critico: Il file {target_file} non esiste nemmeno dopo la clonazione.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Processing delle detections"
      ],
      "metadata": {
        "id": "dbwJKFf8RbhM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# PARTE B: FUNZIONI DI PULIZIA (Detection Cleaning)\n",
        "# ==============================================================================\n",
        "import cv2\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import sys\n",
        "import cv2\n",
        "import torch\n",
        "import torchreid\n",
        "import numpy as np\n",
        "import importlib\n",
        "import re\n",
        "import glob\n",
        "from ultralytics import YOLO\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from types import SimpleNamespace\n",
        "import sys\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchreid import models, metrics\n",
        "from torchreid.losses import TripletLoss, CrossEntropyLoss\n",
        "\n",
        "import os\n",
        "import time\n",
        "import datetime\n",
        "import shutil\n",
        "from sklearn.linear_model import RANSACRegressor\n",
        "\n",
        "import shutil # Import necessario per copiare il file\n",
        "\n",
        "# --- 2. LOGICA RANSAC (Statistica sui Punti) ---\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "def boost_peaks_pixel_level(mask, roi_size=150, dilate_iter=25, peak_sensitivity=20):\n",
        "    \"\"\"\n",
        "    Scansiona il contorno superiore della maschera pixel per pixel alla ricerca di \"punte\"\n",
        "    (minimi locali di Y) e le espande.\n",
        "\n",
        "    Args:\n",
        "        peak_sensitivity: Quanti pixel a dx e sx devono essere 'pi√π bassi'\n",
        "                          per considerare il punto attuale una vera punta.\n",
        "    \"\"\"\n",
        "    h, w = mask.shape\n",
        "    mask_out = mask.copy()\n",
        "\n",
        "    # 1. Trova il contorno principale\n",
        "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE) # NONE = tutti i pixel\n",
        "    if not contours: return mask\n",
        "\n",
        "    cnt = max(contours, key=cv2.contourArea)\n",
        "    points = cnt.reshape(-1, 2) # Array di punti (x, y)\n",
        "\n",
        "    # 2. Identificazione dei \"Picchi\" (Punti visivamente alti = Y Bassa)\n",
        "    # Un punto √® un picco se ha una Y minore (√® pi√π in alto) dei suoi vicini a +/- k\n",
        "\n",
        "    peaks = []\n",
        "    num_points = len(points)\n",
        "\n",
        "    # Per evitare rumore, usiamo un passo di controllo (step)\n",
        "    # Non controlliamo ogni singolo pixel vicino, ma a distanza di 'sensitivity'\n",
        "    step = peak_sensitivity\n",
        "\n",
        "    for i in range(0, num_points, 5): # Saltiamo di 5 in 5 per velocit√†\n",
        "        pt_curr = points[i]\n",
        "        x_curr, y_curr = pt_curr\n",
        "\n",
        "        # Indici dei vicini (gestendo il wrap-around dell'array circolare)\n",
        "        idx_prev = (i - step) % num_points\n",
        "        idx_next = (i + step) % num_points\n",
        "\n",
        "        pt_prev = points[idx_prev]\n",
        "        pt_next = points[idx_next]\n",
        "\n",
        "        # Logica: Sono un picco ALTO se la mia Y √® MINORE dei vicini\n",
        "        # (Ricorda: Y=0 √® il bordo alto dell'immagine)\n",
        "        is_higher_than_prev = y_curr < (pt_prev[1] - 5) # 5px di tolleranza rumore\n",
        "        is_higher_than_next = y_curr < (pt_next[1] - 5)\n",
        "\n",
        "        # Filtro extra: Il picco deve essere nella met√† superiore dell'immagine\n",
        "        # per evitare di prendere i piedi del cameraman come \"picchi\"\n",
        "        is_top_half = y_curr < (h * 0.75)\n",
        "\n",
        "        if is_higher_than_prev and is_higher_than_next and is_top_half:\n",
        "            # Abbiamo trovato una potenziale punta!\n",
        "            peaks.append(pt_curr)\n",
        "\n",
        "    # 3. Consolidamento (Clusterizzazione)\n",
        "    # Spesso un angolo genera 10-20 punti \"picco\" vicini. Ne teniamo uno per gruppo.\n",
        "    valid_corners = []\n",
        "    if len(peaks) > 0:\n",
        "        peaks = np.array(peaks)\n",
        "        # Semplice logica: se due picchi distano meno di 50px, sono lo stesso angolo\n",
        "        # Prendiamo solo il primo di ogni cluster\n",
        "        valid_corners.append(peaks[0])\n",
        "        for i in range(1, len(peaks)):\n",
        "            # Distanza euclidea dall'ultimo aggiunto\n",
        "            dist = np.linalg.norm(peaks[i] - valid_corners[-1])\n",
        "            if dist > 100: # Se dista pi√π di 100px √® un NUOVO angolo\n",
        "                valid_corners.append(peaks[i])\n",
        "\n",
        "    # 4. Applicazione Dilatazione sui Vertici Trovati\n",
        "    for pt in valid_corners:\n",
        "        cx, cy = pt\n",
        "\n",
        "        # Definiamo ROI\n",
        "        x1 = max(0, cx - roi_size)\n",
        "        y1 = max(0, cy - roi_size)\n",
        "        x2 = min(w, cx + roi_size)\n",
        "        y2 = min(h, cy + roi_size)\n",
        "\n",
        "        roi = mask[y1:y2, x1:x2]\n",
        "\n",
        "        if roi.size == 0: continue\n",
        "\n",
        "        # Kernel Ellittico (per fare un 'bozzo' rotondo e naturale)\n",
        "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (9, 9))\n",
        "\n",
        "        # Dilatazione\n",
        "        roi_dilated = cv2.dilate(roi, kernel, iterations=dilate_iter)\n",
        "\n",
        "        # Merge\n",
        "        mask_out[y1:y2, x1:x2] = cv2.bitwise_or(mask_out[y1:y2, x1:x2], roi_dilated)\n",
        "\n",
        "        # [DEBUG VISIVO] Disegna un cerchio rosso nella maschera debug (opzionale)\n",
        "        # cv2.circle(mask_out, (cx, cy), 10, 0, -1) # Buco nero per vederlo\n",
        "\n",
        "    return mask_out\n",
        "\n",
        "def fill_field_holes(mask, max_hole_area=10000):\n",
        "    \"\"\"\n",
        "    Riempe i buchi neri all'interno della maschera bianca se sono pi√π piccoli di max_hole_area.\n",
        "    Serve per coprire arbitri, giocatori o teste in primo piano che bucano la maschera del campo.\n",
        "    \"\"\"\n",
        "    # 1. Copia per non modificare l'originale per riferimento\n",
        "    mask_filled = mask.copy()\n",
        "\n",
        "    # 2. Invertiamo la maschera:\n",
        "    # Ora il CAMPO √® NERO (0) e lo SFONDO/BUCHI sono BIANCHI (255)\n",
        "    inverted_mask = cv2.bitwise_not(mask_filled)\n",
        "\n",
        "    # 3. Troviamo le componenti connesse dell'immagine invertita\n",
        "    # num_labels: quanti oggetti distinti ci sono\n",
        "    # stats: contiene le info su area, bounding box, ecc.\n",
        "    num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(inverted_mask, connectivity=8)\n",
        "\n",
        "    # 4. Identifichiamo lo \"sfondo vero\" (le tribune, l'esterno del campo)\n",
        "    # √à quasi sempre la componente con l'AREA MAGGIORE (escluso lo sfondo 0 che ora √® il campo).\n",
        "    # Se num_labels <= 1 significa che √® tutto campo o tutto sfondo, usciamo.\n",
        "    if num_labels <= 1:\n",
        "        return mask\n",
        "\n",
        "    # Cerchiamo l'indice della label con area massima, escludendo la label 0 (il campo nero)\n",
        "    # stats[1:, cv2.CC_STAT_AREA] prende tutte le aree tranne la prima (background)\n",
        "    # np.argmax(...) ci d√† l'indice relativo, aggiungiamo +1 per tornare all'indice assoluto delle label\n",
        "    largest_label_idx = 1 + np.argmax(stats[1:, cv2.CC_STAT_AREA])\n",
        "\n",
        "    # 5. Iteriamo su tutte le componenti trovate\n",
        "    for i in range(1, num_labels):\n",
        "        # Se questa componente √® lo \"sfondo vero\" (es. le tribune), NON la riempiamo.\n",
        "        if i == largest_label_idx:\n",
        "            continue\n",
        "\n",
        "        area = stats[i, cv2.CC_STAT_AREA]\n",
        "\n",
        "        # Se l'area √® inferiore alla soglia, √® un \"buco spurio\" (persona, ostacolo).\n",
        "        # Lo coloriamo di BIANCO (255) nella maschera ORIGINALE.\n",
        "        # Nota: usiamo labels == i per trovare i pixel di quel buco.\n",
        "        if area < max_hole_area:\n",
        "            mask_filled[labels == i] = 255\n",
        "\n",
        "    return mask_filled\n",
        "\n",
        "def clean_side_with_ransac(points, image_shape, side_name):\n",
        "    \"\"\"\n",
        "    Prende un array di punti (x, y), trova la linea dominante ignorando le sporgenze,\n",
        "    e restituisce i due punti estremi della linea pulita.\n",
        "    \"\"\"\n",
        "    if len(points) < 10: return None # Troppi pochi punti\n",
        "\n",
        "    X = points[:, 0].reshape(-1, 1) # Coordinate x\n",
        "    y = points[:, 1]                # Coordinate y\n",
        "\n",
        "    # RANSAC: Cerca la linea che fitta meglio la maggioranza dei punti\n",
        "    # residual_threshold=10: Se un punto dista pi√π di 10px dalla retta, √® OUTLIER (cartellone)\n",
        "    ransac = RANSACRegressor(residual_threshold=10.0, random_state=42)\n",
        "\n",
        "    try:\n",
        "        ransac.fit(X, y)\n",
        "    except:\n",
        "        return None # Fallimento nel fit\n",
        "\n",
        "    # Recuperiamo gli inlier (i punti che formano la linea \"buona\")\n",
        "    inlier_mask = ransac.inlier_mask_\n",
        "\n",
        "    # Se la linea √® quasi verticale (es. lati laterali), RANSAC su y=mx+q fatica.\n",
        "    # Controllo di sicurezza: se il coefficiente angolare √® folle, gestiamo diversamente.\n",
        "    # Ma per il lato ALTO (orizzontale), questo √® perfetto.\n",
        "\n",
        "    # Calcoliamo i punti estremi della linea predetta estendendola a tutto il frame\n",
        "    line_X = np.array([[0], [image_shape[1]]]) # Da x=0 a x=width\n",
        "    line_y_pred = ransac.predict(line_X)\n",
        "\n",
        "    p1 = (int(line_X[0][0]), int(line_y_pred[0]))\n",
        "    p2 = (int(line_X[1][0]), int(line_y_pred[1]))\n",
        "\n",
        "    return p1, p2\n",
        "\n",
        "def get_field_mask_base(image_bgr):\n",
        "    hsv = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2HSV)\n",
        "    mask = cv2.inRange(hsv, (35, 40, 40), (85, 255, 255))\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, cv2.getStructuringElement(cv2.MORPH_RECT, (15, 15)))\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3)))\n",
        "    num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(mask, connectivity=8)\n",
        "    if num_labels > 1:\n",
        "        largest_label = 1 + np.argmax(stats[1:, cv2.CC_STAT_AREA])\n",
        "        mask = (labels == largest_label).astype(np.uint8) * 255\n",
        "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    cv2.drawContours(mask, contours, -1, 255, thickness=cv2.FILLED)\n",
        "    return mask\n",
        "\n",
        "\n",
        "def get_field_mask_ransac(image_bgr):\n",
        "    # A. Ottieni Maschera Grezza e Contorno\n",
        "    mask = get_field_mask_base(image_bgr)\n",
        "\n",
        "    # --- NUOVO STEP: RIEMPIMENTO BUCHI ---\n",
        "    # Una soglia di 5000-10000 px √® sicura per risoluzioni HD (1920x1080).\n",
        "    # Un giocatore intero occupa meno pixel di uno stadio.\n",
        "    mask = fill_field_holes(mask, max_hole_area=8000)\n",
        "    # -------------------------------------\n",
        "\n",
        "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE) # NONE per avere tutti i punti\n",
        "    if not contours: return mask\n",
        "\n",
        "    cnt = max(contours, key=cv2.contourArea)\n",
        "    points = cnt.reshape(-1, 2) #(N, 2)\n",
        "\n",
        "    # B. Dividi i punti in 4 lati (Nord, Sud, Est, Ovest)\n",
        "    # Usiamo il centroide per determinare la direzione\n",
        "    M = cv2.moments(cnt)\n",
        "    if M[\"m00\"] == 0: return mask\n",
        "    cX = int(M[\"m10\"] / M[\"m00\"])\n",
        "    cY = int(M[\"m01\"] / M[\"m00\"])\n",
        "\n",
        "    top_points = []\n",
        "    bottom_points = []\n",
        "    left_points = []\n",
        "    right_points = []\n",
        "\n",
        "    h, w = mask.shape\n",
        "\n",
        "    for pt in points:\n",
        "        x, y = pt\n",
        "        # Logica semplice basata sulle diagonali del rettangolo immagine\n",
        "        # Dividiamo l'immagine in una \"X\" centrata nel baricentro\n",
        "        # y < cY e distanza verticale > orizzontale -> TOP\n",
        "\n",
        "        dx = x - cX\n",
        "        dy = y - cY\n",
        "\n",
        "        if abs(dx) > abs(dy): # Siamo sui lati Destro/Sinistro\n",
        "            if dx > 0: right_points.append(pt)\n",
        "            else: left_points.append(pt)\n",
        "        else: # Siamo sui lati Alto/Basso\n",
        "            if dy < 0: top_points.append(pt) # Y cresce verso il basso, quindi dy < 0 √® sopra\n",
        "            else: bottom_points.append(pt)\n",
        "\n",
        "    # C. Applichiamo RANSAC solo al lato \"Top\" (quello dei cartelloni)\n",
        "    # Nota: Puoi abilitarlo anche per gli altri se vuoi\n",
        "    top_points = np.array(top_points)\n",
        "\n",
        "    final_mask = mask.copy()\n",
        "\n",
        "    # Disegniamo una maschera di taglio nera sopra le irregolarit√†\n",
        "    if len(top_points) > 50:\n",
        "        line_pts = clean_side_with_ransac(top_points, mask.shape, \"TOP\")\n",
        "\n",
        "        if line_pts:\n",
        "            p1, p2 = line_pts\n",
        "\n",
        "            # Creiamo un poligono che copre tutto ci√≤ che sta SOPRA la linea trovata\n",
        "            # Punti: (0,0) -> (W,0) -> P2 -> P1\n",
        "            cut_poly = np.array([\n",
        "                [0, 0],\n",
        "                [w, 0],\n",
        "                [p2[0], p2[1]],\n",
        "                [p1[0], p1[1]]\n",
        "            ])\n",
        "            cv2.fillPoly(final_mask, [cut_poly], 0) # Riempi di Nero (Taglia via)\n",
        "\n",
        "    # Opzionale: Fallo anche per il lato BASSO per tagliare fotografi\n",
        "    bottom_points = np.array(bottom_points)\n",
        "    if len(bottom_points) > 50:\n",
        "        line_pts_b = clean_side_with_ransac(bottom_points, mask.shape, \"BOTTOM\")\n",
        "        if line_pts_b:\n",
        "            p1, p2 = line_pts_b\n",
        "            # Poligono sotto la linea: P1 -> P2 -> (W, H) -> (0, H)\n",
        "            cut_poly_b = np.array([\n",
        "                [p1[0], p1[1]],\n",
        "                [p2[0], p2[1]],\n",
        "                [w, h],\n",
        "                [0, h]\n",
        "            ])\n",
        "            cv2.fillPoly(final_mask, [cut_poly_b], 0)\n",
        "\n",
        "    final_mask = boost_peaks_pixel_level(final_mask, roi_size=100, dilate_iter=5, peak_sensitivity=200)\n",
        "    final_mask = cv2.dilate(final_mask, None, iterations=2)\n",
        "\n",
        "    return final_mask\n",
        "\n",
        "\n",
        "# --- FUNZIONE DI VERIFICA (Semplice e Veloce) ---\n",
        "def is_feet_in_field(bbox_xywh, field_mask, vertical_margin=5, threshold=0.98):\n",
        "    \"\"\"\n",
        "    Valuta se i piedi sono nel campo calcolando la DENSIT√Ä di pixel bianchi\n",
        "    in una striscia a cavallo del bordo inferiore del Bounding Box.\n",
        "\n",
        "    Args:\n",
        "        vertical_margin: Quanti pixel guardare sopra e sotto (es. 5 -> totale 10px altezza)\n",
        "        threshold: Percentuale minima di bianco richiesta (0.4 = 40%)\n",
        "    \"\"\"\n",
        "    x, y, w, h = map(int, bbox_xywh)\n",
        "    h_img, w_img = field_mask.shape\n",
        "\n",
        "    # --- 1. BORDER SAFETY (Giocatori mezzo busto) ---\n",
        "    # Se il box tocca il fondo dell'immagine, lo salviamo a prescindere dalla maschera.\n",
        "    # Usiamo un margine di tolleranza (es. 10 pixel dal fondo).\n",
        "    bottom_margin = 10\n",
        "    if (y + h) >= (h_img - bottom_margin):\n",
        "        # √à attaccato al fondo: assumiamo sia valido (es. giocatore vicino camera)\n",
        "        return True\n",
        "\n",
        "    # 1. Definizione ROI (Region of Interest)\n",
        "    # Non guardiamo tutta la larghezza, ma il 50% centrale per evitare il background ai lati\n",
        "    roi_w_start = int(x + w * 0.25)\n",
        "    roi_w_end = int(x + w * 0.75)\n",
        "\n",
        "    # Altezza: dal tallone - margin al tallone + margin\n",
        "    y_bottom = y + h\n",
        "    roi_h_start = max(0, y_bottom - vertical_margin)\n",
        "    roi_h_end = min(h_img, y_bottom + vertical_margin)\n",
        "\n",
        "    # Safety check\n",
        "    if roi_w_end <= roi_w_start or roi_h_end <= roi_h_start:\n",
        "        return False\n",
        "\n",
        "    # 2. Estrazione Patch dalla Maschera (Solo 0 o 255)\n",
        "    mask_patch = field_mask[roi_h_start:roi_h_end, roi_w_start:roi_w_end]\n",
        "\n",
        "    # 3. Calcolo Percentuale\n",
        "    white_pixels = cv2.countNonZero(mask_patch)\n",
        "    total_pixels = mask_patch.size # larghezza * altezza\n",
        "\n",
        "    ratio = white_pixels / total_pixels\n",
        "\n",
        "    # DEBUG (Opzionale: stampa per capire i valori)\n",
        "    # print(f\"Ratio: {ratio:.2f}\")\n",
        "\n",
        "    return ratio > threshold\n",
        "\n",
        "\n",
        "def is_shadow_advanced(image_bgr, bbox_xywh):\n",
        "    \"\"\"\n",
        "    Rilevamento Ombre basato su Inversione + Contrasto Hard.\n",
        "    Replica l'effetto visivo: Campo Nero, Ombre Viola Luminose.\n",
        "    \"\"\"\n",
        "    x, y, w, h = map(int, bbox_xywh)\n",
        "\n",
        "    '''# Safety Checks\n",
        "    h_img, w_img = image_bgr.shape[:2]\n",
        "    if w <= 0 or h <= 0: return True\n",
        "    # Margine di sicurezza bordi\n",
        "    margin = 8\n",
        "    if (x <= margin) or (x + w >= w_img - margin) or (y + h >= h_img - margin):\n",
        "        return False'''\n",
        "\n",
        "    # 1. CROP\n",
        "    crop = image_bgr[y:y+h, x:x+w]\n",
        "    if crop.size == 0: return True\n",
        "\n",
        "    # 2. INVERSIONE COLORI\n",
        "    # Il verde scuro (ombra) diventa Magenta Luminoso.\n",
        "    inverted = cv2.bitwise_not(crop)\n",
        "\n",
        "    # 3. CONTRASTO E LUMINOSIT√Ä (La tua richiesta specifica)\n",
        "    # alpha = Contrasto (es. 2.0 raddoppia le differenze)\n",
        "    # beta = Luminosit√† (es. -100 spegne tutto ci√≤ che non √® super luminoso)\n",
        "    alpha = 2.0\n",
        "    beta = -100\n",
        "\n",
        "    # Formula: pixel_new = pixel_old * alpha + beta\n",
        "    contrast_enhanced = cv2.convertScaleAbs(inverted, alpha=alpha, beta=beta)\n",
        "\n",
        "    # 4. ANALISI DEL COLORE RISULTANTE (Viola vs Bianco)\n",
        "    hsv = cv2.cvtColor(contrast_enhanced, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "    # Ora cerchiamo il VIOLA/ROSA che sopravvive al buio.\n",
        "    # Hue del Magenta √® intorno a 150 (range 130-170)\n",
        "    # Saturation deve essere alta (altrimenti √® il bianco della maglia nera invertita)\n",
        "    # Value deve essere alto (deve essere luminoso, altrimenti √® il campo spento)\n",
        "\n",
        "    lower_violet = np.array([130, 60, 60])\n",
        "    upper_violet = np.array([170, 255, 255])\n",
        "\n",
        "    # Maschera dell'ombra\n",
        "    shadow_mask = cv2.inRange(hsv, lower_violet, upper_violet)\n",
        "\n",
        "    # Maschera del \"Giocatore Nero\" (che invertito √® diventato Bianco/Grigio Chiaro)\n",
        "    # Bassa saturazione, Alta luminosit√†\n",
        "    lower_white = np.array([0, 0, 150])\n",
        "    upper_white = np.array([180, 50, 255])\n",
        "    player_white_mask = cv2.inRange(hsv, lower_white, upper_white)\n",
        "\n",
        "    # Maschera \"Altri Colori\" (Giocatori Colorati -> Colori complementari luminosi)\n",
        "    # Tutto ci√≤ che √® luminoso (>60) ma NON √® viola e NON √® bianco\n",
        "    bright_mask = cv2.inRange(hsv, np.array([0, 0, 60]), np.array([180, 255, 255]))\n",
        "    # Sottraiamo viola e bianco\n",
        "    other_features_mask = cv2.bitwise_and(bright_mask, bright_mask, mask=cv2.bitwise_not(shadow_mask))\n",
        "    other_features_mask = cv2.bitwise_and(other_features_mask, other_features_mask, mask=cv2.bitwise_not(player_white_mask))\n",
        "\n",
        "    # --- CONTEGGI ---\n",
        "    total_pixels = crop.shape[0] * crop.shape[1]\n",
        "    count_shadow = cv2.countNonZero(shadow_mask)\n",
        "    count_player = cv2.countNonZero(player_white_mask) + cv2.countNonZero(other_features_mask)\n",
        "\n",
        "    # --- DECISIONE ---\n",
        "\n",
        "    # Se vediamo pixel di \"Giocatore\" (Bianco o Colore acceso non viola)\n",
        "    # Basta poco, perch√© il giocatore √® solido.\n",
        "    if count_player > total_pixels * 0.10:\n",
        "        return False # Trovato struttura di giocatore\n",
        "\n",
        "    # Se la maggior parte di ci√≤ che brilla √® Viola\n",
        "    # Nota: col contrasto alto, il campo \"vuoto\" diventa nero (0,0,0), quindi non conta.\n",
        "    # Contiamo solo ci√≤ che √® emerso dal buio.\n",
        "    valid_pixels = count_shadow + count_player\n",
        "    if valid_pixels == 0:\n",
        "        return True # Se √® tutto nero, era erba piatta -> Ombra/Scarto\n",
        "\n",
        "    ratio_shadow = count_shadow / float(valid_pixels)\n",
        "\n",
        "    if ratio_shadow > 0.80:\n",
        "        return True # √à quasi solo viola -> Ombra\n",
        "\n",
        "    return False\n",
        "\n",
        "def batch_shadow_filtering(detections_xywh, image_bgr, anchor_tolerance=8):\n",
        "    \"\"\"\n",
        "    Pipeline completa per rimozione sovrapposizioni e ombre.\n",
        "\n",
        "    FASE 1: Filtro intrinseco (is_shadow_advanced) per rimuovere ombre scure/senza texture.\n",
        "    FASE 2: Filtro geometrico 'NMS sui piedi'. Ordina per AREA.\n",
        "            Se due box condividono i piedi (angoli inferiori), MANTIENE IL PI√ô GRANDE\n",
        "            e rimuove il pi√π piccolo (che sia un'ombra o un pezzo di corpo).\n",
        "    \"\"\"\n",
        "    if not detections_xywh:\n",
        "        return []\n",
        "\n",
        "    h_img, w_img = image_bgr.shape[:2]\n",
        "\n",
        "    # --- FASE 1: FILTRO INTRINSECO (Analisi Singola) ---\n",
        "    survivors_phase_1 = []\n",
        "\n",
        "    for i, bbox in enumerate(detections_xywh):\n",
        "        # Chiamiamo la funzione singola (filtro texture/colore)\n",
        "        is_bad = is_shadow_advanced(image_bgr, bbox)\n",
        "\n",
        "        if not is_bad:\n",
        "            x, y, w, h = map(int, bbox)\n",
        "            x1, y1 = max(0, x), max(0, y)\n",
        "            x2, y2 = min(w_img, x + w), min(h_img, y + h)\n",
        "            area = w * h\n",
        "\n",
        "            survivors_phase_1.append({\n",
        "                'box': bbox,\n",
        "                'coords': (x1, y1, x2, y2), # x1, y1, x2, y2\n",
        "                'area': area,\n",
        "                'original_idx': i\n",
        "            })\n",
        "\n",
        "    # --- FASE 2: FILTRO RELAZIONALE (Priorit√† al Grande) ---\n",
        "\n",
        "    # 1. ORDINAMENTO DECRESCENTE PER AREA\n",
        "    # Fondamentale: mettiamo i box pi√π grandi all'inizio della lista.\n",
        "    survivors_phase_1.sort(key=lambda x: x['area'], reverse=True)\n",
        "\n",
        "    indices_to_remove_phase_2 = set()\n",
        "    num_survivors = len(survivors_phase_1)\n",
        "\n",
        "    for i in range(num_survivors):\n",
        "        # Se i √® gi√† stato rimosso (perch√© era pi√π piccolo di uno precedente), saltalo\n",
        "        if i in indices_to_remove_phase_2:\n",
        "            continue\n",
        "\n",
        "        cand_A = survivors_phase_1[i] # Box \"Dominante\" (Il pi√π grande attuale)\n",
        "        xA_1, yA_1, xA_2, yA_2 = cand_A['coords']\n",
        "\n",
        "        # Confrontiamo solo con i successivi (j > i), che sono sicuramente PI√ô PICCOLI o uguali\n",
        "        for j in range(i + 1, num_survivors):\n",
        "            if j in indices_to_remove_phase_2:\n",
        "                continue\n",
        "\n",
        "            cand_B = survivors_phase_1[j] # Box \"Piccolo\" (Candidato alla rimozione)\n",
        "            xB_1, yB_1, xB_2, yB_2 = cand_B['coords']\n",
        "\n",
        "            # LOGICA GEOMETRICA SUGLI ANGOLI INFERIORI (PIEDI)\n",
        "\n",
        "            # 1. Controllo asse Y (Altezza piedi simile)\n",
        "            if abs(yA_2 - yB_2) <= anchor_tolerance:\n",
        "\n",
        "                # 2. Controllo asse X (Angolo SX o Angolo DX coincidenti)\n",
        "                dist_left = abs(xA_1 - xB_1)\n",
        "                dist_right = abs(xA_2 - xB_2)\n",
        "\n",
        "                if dist_left <= anchor_tolerance or dist_right <= anchor_tolerance:\n",
        "                    # Abbiamo trovato un box B pi√π piccolo che \"nasce\" dagli stessi piedi di A.\n",
        "                    # Poich√© A √® pi√π grande (garantito dal sort), A √® il \"Corpo Intero\".\n",
        "                    # B √® un frammento (busto) o un'ombra inclusa.\n",
        "                    # RIMUOVIAMO B.\n",
        "                    indices_to_remove_phase_2.add(j)\n",
        "\n",
        "    # --- COSTRUZIONE OUTPUT ---\n",
        "    final_detections = []\n",
        "    for i in range(num_survivors):\n",
        "        if i not in indices_to_remove_phase_2:\n",
        "            final_detections.append(survivors_phase_1[i]['box'])\n",
        "\n",
        "    return final_detections\n",
        "\n",
        "\n",
        "def calculate_iou(box1, box2):\n",
        "    \"\"\"\n",
        "    Calcola IoU tra due box in formato (x, y, w, h)\n",
        "    \"\"\"\n",
        "    x1, y1, w1, h1 = box1\n",
        "    x2, y2, w2, h2 = box2\n",
        "\n",
        "    # Coordinate angoli\n",
        "    xA = max(x1, x2)\n",
        "    yA = max(y1, y2)\n",
        "    xB = min(x1 + w1, x2 + w2)\n",
        "    yB = min(y1 + h1, y2 + h2)\n",
        "\n",
        "    interWidth = max(0, xB - xA)\n",
        "    interHeight = max(0, yB - yA)\n",
        "    interArea = interWidth * interHeight\n",
        "\n",
        "    box1Area = w1 * h1\n",
        "    box2Area = w2 * h2\n",
        "\n",
        "    unionArea = box1Area + box2Area - interArea\n",
        "    if unionArea == 0: return 0\n",
        "\n",
        "    return interArea / unionArea\n",
        "\n",
        "def clean_blur_artifacts(detections, confidences,\n",
        "                         iou_thresh=0.4,\n",
        "                         vertical_tol=3,\n",
        "                         conf_target_thresh=0.40):\n",
        "    \"\"\"\n",
        "    Rimuove le rilevazioni duplicate causate dal motion blur orizzontale.\n",
        "\n",
        "    Logica:\n",
        "    1. Ordina per confidenza.\n",
        "    2. Se due box hanno IoU > 0.4 AND altezze identiche (Ymin e Ymax uguali entro tolleranza).\n",
        "    3. Elimina quello con confidenza minore SE la sua confidenza √® sotto la soglia di rischio (40%).\n",
        "\n",
        "    Args:\n",
        "        detections: lista di list/array [x, y, w, h]\n",
        "        confidences: lista di float [conf] corrispondente\n",
        "        iou_thresh: IoU minimo per considerare la sovrapposizione (default 0.4)\n",
        "        vertical_tol: Tolleranza in pixel per l'allineamento verticale (default 3px)\n",
        "        conf_target_thresh: Agisce solo se la vittima ha confidenza < 0.40\n",
        "    \"\"\"\n",
        "\n",
        "    if len(detections) == 0:\n",
        "        return [], []\n",
        "\n",
        "    # Creiamo una lista strutturata per ordinare mantenendo gli indici originali\n",
        "    # Format: {'bbox': [x,y,w,h], 'conf': 0.xyz, 'keep': True}\n",
        "    dets = []\n",
        "    for i, (bbox, conf) in enumerate(zip(detections, confidences)):\n",
        "        dets.append({\n",
        "            'bbox': bbox,\n",
        "            'conf': conf,\n",
        "            'keep': True,\n",
        "            'y_min': bbox[1],\n",
        "            'y_max': bbox[1] + bbox[3]\n",
        "        })\n",
        "\n",
        "    # Ordina per confidenza decrescente (Il pi√π sicuro decide chi eliminare)\n",
        "    dets.sort(key=lambda x: x['conf'], reverse=True)\n",
        "\n",
        "    for i in range(len(dets)):\n",
        "        if not dets[i]['keep']:\n",
        "            continue\n",
        "\n",
        "        base = dets[i]\n",
        "\n",
        "        for j in range(i + 1, len(dets)):\n",
        "            if not dets[j]['keep']:\n",
        "                continue\n",
        "\n",
        "            cand = dets[j]\n",
        "\n",
        "            # 1. CONTROLLO RAPIDO VERTICALE (Il cuore della tua idea)\n",
        "            # Verifica allineamento Y_MIN e Y_MAX\n",
        "            diff_ymin = abs(base['y_min'] - cand['y_min'])\n",
        "            diff_ymax = abs(base['y_max'] - cand['y_max'])\n",
        "\n",
        "            # Se non sono allineati verticalmente quasi al pixel, non √® blur orizzontale\n",
        "            if diff_ymin > vertical_tol or diff_ymax > vertical_tol:\n",
        "                continue\n",
        "\n",
        "            # 2. CONTROLLO SICUREZZA\n",
        "            # Eliminiamo solo se il candidato (che ha confidenza minore di base)\n",
        "            # √® effettivamente \"incerto\" (< 0.40).\n",
        "            # Se entrambi hanno 0.80 e 0.75, probabilmente sono due giocatori veri vicini.\n",
        "            if cand['conf'] >= conf_target_thresh:\n",
        "                continue\n",
        "\n",
        "            # 3. CONTROLLO IoU\n",
        "            # Se sono allineati verticalmente e si sovrappongono abbastanza\n",
        "            iou = calculate_iou(base['bbox'], cand['bbox'])\n",
        "\n",
        "            if iou > iou_thresh:\n",
        "                # √à un ghost da blur!\n",
        "                dets[j]['keep'] = False\n",
        "\n",
        "    # Ricostruisce le liste finali\n",
        "    final_dets = [d['bbox'] for d in dets if d['keep']]\n",
        "    final_confs = [d['conf'] for d in dets if d['keep']]\n",
        "\n",
        "    return final_dets, final_confs"
      ],
      "metadata": {
        "id": "m8tfhGvURhfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCC-qCLAibpO"
      },
      "source": [
        "# YOLO/RT-DETR + HybridOSNet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "esegui pipeline"
      ],
      "metadata": {
        "id": "IRcrlBiN5Slo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4tJ8qJ8if_k"
      },
      "outputs": [],
      "source": [
        "if './yolo_tracking' not in sys.path:\n",
        "    sys.path.insert(0, './yolo_tracking')\n",
        "\n",
        "try:\n",
        "    from boxmot.trackers.botsort.botsort import BotSort\n",
        "    BoTSORT = BotSort  # Creiamo l'alias necessario\n",
        "    print(\"‚úÖ BoTSORT importato e pronto.\")\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è Tentativo importazione fallito. Riprovo con path assoluto...\")\n",
        "    # Tentativo disperato se il primo fallisce\n",
        "    sys.path.append('/content/yolo_tracking/boxmot')\n",
        "    from boxmot.trackers.botsort.botsort import BotSort\n",
        "    BoTSORT = BotSort\n",
        "\n",
        "\n",
        "# --- 1. CONFIGURAZIONE UTENTE ---\n",
        "VIDEO_SEQ = \"1\"   # <--- Nome cartella (es. \"4\", senza zeri iniziali se hai rinominato)\n",
        "GROUP_ID = \"13\"   # <--- Il tuo numero di gruppo\n",
        "\n",
        "# Percorsi aggiornati per essere relativi alla cartella del progetto\n",
        "DATASET_ROOT = './dataset/videos'\n",
        "OUTPUT_RESULTS_DIR = './Predictions_folder'\n",
        "YOLO_WEIGHTS = './runs/RT-Detr/weights/best.pt'\n",
        "REID_WEIGHTS = './runs/HybridOSNet-CNN_Transformer/model/model_best.pth.tar'\n",
        "\n",
        "\n",
        "\n",
        "class VisionTransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, heads=4, height=16, width=8, dropout=0.1, debug_freq=20):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.heads = heads\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "\n",
        "        # --- AGGIUNTA PER DEBUG ---\n",
        "        self.debug_freq = debug_freq  # Ogni quanti batch stampare\n",
        "        self.batch_counter = 0        # Contatore interno\n",
        "        # --------------------------\n",
        "\n",
        "        # Proiezioni Q, K, V usando Linear (come il prof)\n",
        "        # Nota: Linear richiede input (Batch, Seq_Len, Dim)\n",
        "        self.query_projection = nn.Linear(dim, dim)\n",
        "        self.key_projection = nn.Linear(dim, dim)\n",
        "        self.value_projection = nn.Linear(dim, dim)\n",
        "        self.output_projection = nn.Linear(dim, dim)\n",
        "\n",
        "        # Positional Embedding IMPARABILE (stile ViT)\n",
        "        # Invece di quello sinusoidale del prof (ottimo per testo),\n",
        "        # per le immagini fisse conviene lasciare che la rete impari la posizione.\n",
        "        self.num_patches = height * width\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches, dim))\n",
        "\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Input x: (Batch, Channel, Height, Width) -> Es. (B, 512, 16, 8)\n",
        "        \"\"\"\n",
        "\n",
        "        # --- LOGICA DI DEBUG PERIODICA ---\n",
        "        if self.training:\n",
        "            self.batch_counter += 1\n",
        "            # Stampa solo se il contatore √® multiplo della frequenza scelta\n",
        "            if self.batch_counter % self.debug_freq == 0:\n",
        "                print(f\"üîç [ViT Heartbeat] Batch {self.batch_counter} | Input Shape: {x.shape} (Atteso H={self.height}, W={self.width})\")\n",
        "        # ---------------------------------\n",
        "\n",
        "        B, C, H, W = x.shape\n",
        "\n",
        "        # 1. FLATTEN: Trasformiamo l'immagine 2D in una sequenza 1D\n",
        "        # Da (B, C, H, W) -> (B, H*W, C)\n",
        "        # .permute(0, 2, 3, 1) mette i canali alla fine\n",
        "        # .flatten(1, 2) schiaccia H e W insieme\n",
        "        x_flat = x.permute(0, 2, 3, 1).flatten(1, 2) # (B, SeqLen, Dim)\n",
        "\n",
        "        # 2. Aggiunta Positional Embedding\n",
        "        # Se le dimensioni cambiano dinamicamente, interpoliamo il pos embedding\n",
        "        if x_flat.shape[1] != self.pos_embedding.shape[1]:\n",
        "             # Gestione sicurezza ridimensionamento\n",
        "             pos_emb = F.interpolate(\n",
        "                 self.pos_embedding.permute(0, 2, 1).view(1, C, self.height, self.width),\n",
        "                 size=(H, W), mode='bilinear').flatten(2).permute(0, 2, 1)\n",
        "             x_flat = x_flat + pos_emb\n",
        "        else:\n",
        "             x_flat = x_flat + self.pos_embedding\n",
        "\n",
        "        # 3. Layer Norm prima dell'attention (Pre-Norm architecture, pi√π stabile)\n",
        "        residual = x_flat\n",
        "        x_norm = self.norm(x_flat)\n",
        "\n",
        "        # 4. Proiezioni Lineari\n",
        "        Q = self.query_projection(x_norm)\n",
        "        K = self.key_projection(x_norm)\n",
        "        V = self.value_projection(x_norm)\n",
        "\n",
        "        # 5. Split Heads (come nel codice del prof, ma ottimizzato con view)\n",
        "        # (B, SeqLen, Heads, HeadDim) -> (B, Heads, SeqLen, HeadDim)\n",
        "        Q = Q.view(B, -1, self.heads, C // self.heads).transpose(1, 2)\n",
        "        K = K.view(B, -1, self.heads, C // self.heads).transpose(1, 2)\n",
        "        V = V.view(B, -1, self.heads, C // self.heads).transpose(1, 2)\n",
        "\n",
        "        # 6. SCALED DOT PRODUCT ATTENTION (Il cuore ottimizzato)\n",
        "        # is_causal=False perch√© nell'immagine guardiamo tutto il contesto\n",
        "        out = F.scaled_dot_product_attention(Q, K, V, dropout_p=0.1 if self.training else 0.0, is_causal=False)\n",
        "\n",
        "        # 7. Join Heads\n",
        "        out = out.transpose(1, 2).contiguous().view(B, -1, C)\n",
        "\n",
        "        # 8. Output Projection + Residual\n",
        "        out = self.output_projection(out)\n",
        "        out = out + residual\n",
        "\n",
        "        # 9. UNFLATTEN: Torniamo a immagine 2D\n",
        "        # Da (B, SeqLen, C) -> (B, C, H, W)\n",
        "        out = out.permute(0, 2, 1).view(B, C, H, W)\n",
        "\n",
        "        return out\n",
        "\n",
        "# --- 1. Moduli Base (CBAM, GeM) restano uguali ---\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, in_planes, ratio=16):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "\n",
        "        # 1. Average Pooling\n",
        "        # Riduce (H, W) a (1, 1) facendo la media.\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        # 2. Max Pooling\n",
        "        # Riduce (H, W) a (1, 1) prendendo il valore massimo.\n",
        "        # Serve a preservare le feature di texture pi√π forti.\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "\n",
        "        # 3. COMPRESSIONE (Shared MLP)\n",
        "        # Questo √® il primo strato del \"bottleneck\" che riduce i canali.\n",
        "        # Usa Conv2d con kernel 1x1 che equivale a un Fully Connected layer.\n",
        "        self.fc1 = nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False)\n",
        "        self.relu1 = nn.ReLU()\n",
        "\n",
        "        # 4. RIESPANSIONE (Shared MLP)\n",
        "        # Ripristina il numero originale di canali.\n",
        "        self.fc2 = nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n",
        "\n",
        "        # 5. GATING (Sigmoide)\n",
        "        # Prepara l'attivazione tra 0 e 1 per pesare i canali.\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Le stesse fc1 e fc2 vengono usate SIA per il vettore delle medie SIA per quello dei massimi.\n",
        "\n",
        "        # Percorso A: Quello che descrivevi tu (Avg -> MLP)\n",
        "\n",
        "        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n",
        "\n",
        "        # Percorso B: L'aggiunta del CBAM (Max -> MLP)\n",
        "        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n",
        "\n",
        "        # a questo punto il tensore in ingresso √® stato trasformato in due vettori monodimensionali, lunghi la dimensione dei canali.\n",
        "\n",
        "        # FUSIONE\n",
        "        # I due vettori vengono sommati element-wise.\n",
        "        out = avg_out + max_out\n",
        "\n",
        "        # ATTENZIONE:\n",
        "        # Questo blocco restituisce SOLO i pesi (la maschera di attenzione),\n",
        "        # NON esegue la moltiplicazione finale per l'input 'x'.\n",
        "\n",
        "        return self.sigmoid(out) #restituisce vettore con valori di attention per ogni feature map del tensore x.\n",
        "\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super(SpatialAttention, self).__init__()\n",
        "        # Per mantenere le dimensioni H x W inalterate dopo una conv 7x7,\n",
        "        # serve un padding di 3. (formula: p = (k-1)/2)\n",
        "        padding = 3 if kernel_size == 7 else 1\n",
        "\n",
        "        # Questa convoluzione ridurr√† i 2 canali di input (Max+Avg) a 1 canale di output.\n",
        "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x √® il tuo tensore di input [Batch, Canali, Altezza, Larghezza]\n",
        "\n",
        "        # 1. COMPRESSIONE DEI CANALI (Average Pooling)\n",
        "        # Invece di fare la media spaziale (come nel blocco Channel), qui facciamo la media SUI CANALI.\n",
        "        # \"In media, quanto √® attiva questa posizione (h,w) su tutti i filtri?\"\n",
        "        # Output: [B, 1, H, W]\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
        "\n",
        "        # 2. COMPRESSIONE DEI CANALI (Max Pooling)\n",
        "        # \"Qual √® la feature pi√π forte in assoluto in questa posizione?\"\n",
        "        # Questo √® utilissimo per trovare i bordi o dettagli unici del giocatore.\n",
        "        # Output: [B, 1, H, W]\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "\n",
        "        # 3. CONCATENAZIONE\n",
        "        # Uniamo le due mappe. Ora abbiamo una rappresentazione \"grezza\" di dove si trovano le informazioni.\n",
        "        # Output: [B, 2, H, W]\n",
        "        x = torch.cat([avg_out, max_out], dim=1)\n",
        "\n",
        "        # 4. CONVOLUZIONE SPAZIALE\n",
        "        # Qui sta l'intelligenza locale. Un kernel grande (7x7) scorre sull'immagine.\n",
        "        # Impara a capire che se c'√® un picco di attivazione in un punto,\n",
        "        # probabilmente √® importante anche l'area subito vicina.\n",
        "        # Trasforma i 2 canali in 1 solo canale di \"importanza spaziale\".\n",
        "        # Output: [B, 1, H, W]\n",
        "        x = self.conv1(x)\n",
        "\n",
        "        # 5. GENERAZIONE DELLA MASCHERA di dimensione [B, 1, H, W]\n",
        "        # Schiaccia i valori tra 0 e 1.\n",
        "        # 1 = \"Questa zona √® importante (il giocatore)\"\n",
        "        # 0 = \"Questa zona √® rumore (il campo)\"\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "class CBAM(nn.Module):\n",
        "    def __init__(self, in_planes, ratio=16, kernel_size=7):\n",
        "        super(CBAM, self).__init__()\n",
        "        self.ca = ChannelAttention(in_planes, ratio)\n",
        "        self.sa = SpatialAttention(kernel_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1. CHANNEL ATTENTION\n",
        "        out = x * self.ca(x) # Moltiplica l'input per i pesi dei canali\n",
        "\n",
        "        # 2. SPATIAL ATTENTION (applicata sull'output del channel)\n",
        "        result = out * self.sa(out) # Moltiplica il risultato raffinato per la maschera spaziale\n",
        "\n",
        "        return result\n",
        "\n",
        "class GeM(nn.Module):\n",
        "    def __init__(self, p=3.0, eps=1e-6):\n",
        "        super(GeM, self).__init__()\n",
        "\n",
        "        # 1. PARAMETRO P ADDESTRABILE\n",
        "        # Qui sta la magia. Non definiamo p come una costante (self.p = p),\n",
        "        # ma come un nn.Parameter.\n",
        "        # Questo dice a PyTorch: \"Durante la backpropagation, aggiorna anche questo valore\n",
        "        # per minimizzare la loss\".\n",
        "        # Inizializziamo a 3.0 perch√© √® un buon punto di partenza empirico per il ReID.\n",
        "        self.p = nn.Parameter(torch.ones(1) * p)\n",
        "\n",
        "        # 2. EPSILON\n",
        "        # Un valore piccolissimo per evitare divisioni per zero o radici di numeri instabili.\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: Tensore di input [Batch, Channels, Height, Width]\n",
        "\n",
        "        # 3. CLAMPING (Sicurezza Numerica)\n",
        "        # x.clamp(min=self.eps) forza tutti i valori nel tensore a essere almeno 1e-6.\n",
        "        # Evita NaN (Not a Number) durante il training.\n",
        "\n",
        "        # 4. ELEVAMENTO A POTENZA\n",
        "        # .pow(self.p) eleva ogni singolo pixel alla potenza p (che la rete sta imparando).\n",
        "        # Se p > 1, questo enfatizza i valori alti (i dettagli salienti del calciatore)\n",
        "        # e schiaccia verso zero i valori bassi (lo sfondo).\n",
        "\n",
        "        # 5. GLOBAL AVERAGE POOLING\n",
        "        # F.avg_pool2d(...) calcola la media spaziale.\n",
        "        # Il kernel size √® (x.size(-2), x.size(-1)), ovvero (Height, Width).\n",
        "        # Questo significa: \"Prendi tutta la feature map HxW e fanne una media unica\".\n",
        "        # Output parziale: [Batch, Channels, 1, 1]\n",
        "\n",
        "        # 6. RADICE P-ESIMA (Inverse Power)\n",
        "        # .pow(1. / self.p) applica la radice p-esima.\n",
        "        # Serve a riportare i valori alla scala originale (dopo averli elevati alla p prima della media).\n",
        "        return F.avg_pool2d(x.clamp(min=self.eps).pow(self.p), (x.size(-2), x.size(-1))).pow(1. / self.p)\n",
        "\n",
        "\n",
        "# --- 2. Modello HPM con Output Multipli ---\n",
        "class SoccerNetHybridModel(nn.Module):\n",
        "    def __init__(self, num_classes, model_name='osnet_x1_0', loss='triplet', best_weights_path=None):\n",
        "        super(SoccerNetHybridModel, self).__init__()\n",
        "        self.loss = loss\n",
        "\n",
        "        print(f\"üèóÔ∏è HYBRID Model 2.0: Backbone={model_name} + CBAM + VisionTransformer + GeM\")\n",
        "\n",
        "        # Caricamento del backbone OSNet ed eliminazione del classificatore.\n",
        "        # 1. Costruisci Backbone VUOTO (pretrained=False perch√© carichiamo i nostri)\n",
        "        base_model = models.build_model(name=model_name, num_classes=num_classes, pretrained=False, loss='triplet')\n",
        "\n",
        "        # --- CARICAMENTO PESI CUSTOM (WARM-UP) ---\n",
        "        if best_weights_path and os.path.exists(best_weights_path):\n",
        "            print(f\"‚ôªÔ∏è Caricamento pesi Backbone da: {best_weights_path}\")\n",
        "            checkpoint = torch.load(best_weights_path, weights_only=False)\n",
        "            state_dict = checkpoint['state_dict']\n",
        "\n",
        "            # Pulizia delle chiavi (Rimuove 'module.' e layer non backbone)\n",
        "            new_state_dict = {}\n",
        "            for k, v in state_dict.items():\n",
        "                k = k.replace(\"module.\", \"\") # Gestione DataParallel\n",
        "                # Carichiamo solo la parte feature extractor, ignoriamo classifier/fc vecchi\n",
        "                if not k.startswith(\"classifier\") and not k.startswith(\"fc\"):\n",
        "                    new_state_dict[k] = v\n",
        "\n",
        "            # Carica con strict=False per ignorare le chiavi mancanti (classifier)\n",
        "            base_model.load_state_dict(new_state_dict, strict=False)\n",
        "            print(\"‚úÖ Pesi Backbone caricati con successo!\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è Nessun peso custom fornito o file non trovato. Uso inizializzazione casuale.\")\n",
        "        # -----------------------------------------\n",
        "\n",
        "\n",
        "        self.backbone = base_model\n",
        "        if hasattr(self.backbone, 'classifier'): del self.backbone.classifier\n",
        "        if hasattr(self.backbone, 'fc'): del self.backbone.fc\n",
        "\n",
        "        # caricamento dei moduli CBAM e GeM\n",
        "        self.in_channels = 512\n",
        "        self.cbam = CBAM(self.in_channels)\n",
        "\n",
        "        # TRANSFORMER BLOCK (Attention Globale - Relazioni a lungo raggio)\n",
        "        # OSNet riduce le dimensioni di 16x.\n",
        "        # Se input standard ReID (256x128) -> Feature map (16x8)\n",
        "        self.trans = VisionTransformerBlock(self.in_channels, heads=4, height=16, width=8)\n",
        "\n",
        "        self.gem = GeM()\n",
        "\n",
        "        # 3. Head Singola\n",
        "        self.bn = nn.BatchNorm1d(self.in_channels)\n",
        "        self.bn.bias.requires_grad_(False)\n",
        "        self.bn.apply(self._weights_init_kaiming)\n",
        "\n",
        "        self.classifier = nn.Linear(self.in_channels, num_classes, bias=False)\n",
        "        self.classifier.apply(self._weights_init_classifier)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x, return_featuremaps=True)\n",
        "        # Output atteso: (B, 512, H, W)\n",
        "\n",
        "        features = self.cbam(features)\n",
        "        # CBAM preserva le dimensioni\n",
        "\n",
        "        # 3. TRANSFORMER (Collega le parti del corpo)\n",
        "        features = self.trans(features)\n",
        "\n",
        "        # Pooling (da H,W a 1,1) -> Flatten\n",
        "        global_features = self.gem(features).view(features.size(0), -1)\n",
        "\n",
        "        # Normalization\n",
        "        feat_norm = self.bn(global_features)\n",
        "\n",
        "        if self.training:\n",
        "            logits = self.classifier(feat_norm)\n",
        "            # Ritorna: (logits, features) per le due loss\n",
        "            if self.loss == 'triplet':\n",
        "                return logits, feat_norm\n",
        "            return logits\n",
        "        else:\n",
        "            # In inferenza: solo il vettore normalizzato\n",
        "            return feat_norm\n",
        "\n",
        "    def _weights_init_kaiming(self, m):\n",
        "        if isinstance(m, nn.BatchNorm1d):\n",
        "            nn.init.constant_(m.weight, 1.0); nn.init.constant_(m.bias, 0.0)\n",
        "    def _weights_init_classifier(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.normal_(m.weight, std=0.001);\n",
        "            if m.bias is not None: nn.init.constant_(m.bias, 0.0)\n",
        "\n",
        "# ==============================================================================\n",
        "# PARTE A: CLASSE REID EXTRACTOR (Il tuo modello custom)\n",
        "# ==============================================================================\n",
        "class ReIDExtractor:\n",
        "    def __init__(self, model_path, model_name='osnet_x1_0', device='0'):\n",
        "        # Logica Device Robusta\n",
        "        if device == '0' or device == 'cuda':\n",
        "            self.device = torch.device('cuda')\n",
        "        else:\n",
        "            self.device = torch.device('cpu')\n",
        "        print(f\"üîÑ Init ReID: {model_name}...\")\n",
        "        # 1. ISTANZIAZIONE DEL MODELLO CUSTOM\n",
        "        # Passiamo num_classes=1000 (fittizio) perch√© in inferenza non usiamo il classificatore.\n",
        "        # Passiamo best_weights_path=None perch√© carichiamo i pesi COMPLETI subito dopo.\n",
        "        self.model = SoccerNetHybridModel(\n",
        "            num_classes=1000,\n",
        "            model_name=model_name,\n",
        "            loss='triplet',\n",
        "            best_weights_path=None\n",
        "        )\n",
        "\n",
        "        # 2. CARICAMENTO DEI PESI (Full Hybrid Model)\n",
        "        if model_path and os.path.exists(model_path):\n",
        "            print(f\"üì• Caricamento pesi modello ibrido da: {os.path.basename(model_path)}\")\n",
        "            try:\n",
        "                # Carica il checkpoint\n",
        "                checkpoint = torch.load(model_path, map_location=self.device, weights_only = False)\n",
        "\n",
        "                # Gestione se i pesi sono dentro una chiave 'state_dict' (tipico di torchreid)\n",
        "                state_dict = checkpoint['state_dict'] if 'state_dict' in checkpoint else checkpoint\n",
        "\n",
        "                # --- FIX CRITICO: FILTRAGGIO MANUALE ---\n",
        "                clean_state_dict = {}\n",
        "                for k, v in state_dict.items():\n",
        "                    name = k.replace('module.', '') # Rimuove prefisso DataParallel\n",
        "\n",
        "                    # SE IL NOME CONTIENE 'classifier' o 'fc', LO BUTTIAMO VIA\n",
        "                    if 'classifier' in name or 'fc' in name:\n",
        "                        continue\n",
        "\n",
        "                    clean_state_dict[name] = v\n",
        "\n",
        "                # Carichiamo solo i pesi filtrati (Backbone + CBAM + ViT + GeM)\n",
        "                # strict=False √® fondamentale perch√© mancher√† il classifier, ma a noi va bene cos√¨!\n",
        "                self.model.load_state_dict(clean_state_dict, strict=False)\n",
        "\n",
        "                print(\"‚úÖ SUCCESSO: Pesi caricati correttamente (Classifier rimosso, size mismatch evitato).\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Errore critico nel caricamento pesi: {e}\")\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è WARNING: File pesi non trovato: {model_path}. Il modello √® inizializzato a caso!\")\n",
        "\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((256, 128)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "\n",
        "    def extract_features(self, crops):\n",
        "        if len(crops) == 0: return np.empty((0, 512))\n",
        "        batch = []\n",
        "        for img in crops:\n",
        "            if isinstance(img, np.ndarray):\n",
        "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "                img = Image.fromarray(img)\n",
        "            batch.append(self.transform(img))\n",
        "        batch = torch.stack(batch).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            features = self.model(batch)\n",
        "        return features.cpu().numpy()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# PARTE C: PIPELINE DI TRACKING\n",
        "# ==============================================================================\n",
        "def run_tracking_pipeline(video_seq, group_id):\n",
        "    # 1. Carica Modelli\n",
        "    print(f\"üèóÔ∏è Caricamento YOLO: {os.path.basename(YOLO_WEIGHTS)}\")\n",
        "    yolo_model = YOLO(YOLO_WEIGHTS)\n",
        "    reid_extractor = ReIDExtractor(model_path=REID_WEIGHTS, model_name='osnet_x1_0')\n",
        "\n",
        "    device = '0' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    # --- TRUCCO ANTI-CRASH (Il \"Trojan Horse\") ---\n",
        "    # Il problema: BoTSORT esige che il file dei pesi abbia un nome tipo \"modello_dataset.pt\".\n",
        "    # Se il tuo file si chiama \"best.pt\", la libreria crasha cercando di leggere il nome del dataset.\n",
        "    # Soluzione: Copiamo temporaneamente i tuoi pesi con un nome che piace a BoTSORT (\"..._market1501.pt\").\n",
        "\n",
        "    temp_weights = '/content/osnet_x1_0_market1501.pt' # Nome fittizio \"corretto\"\n",
        "\n",
        "    # Usiamo i tuoi pesi se esistono, altrimenti quelli di YOLO come tappabuchi\n",
        "    source = REID_WEIGHTS if os.path.exists(REID_WEIGHTS) else YOLO_WEIGHTS\n",
        "    shutil.copy(source, temp_weights)\n",
        "    print(f\"üé≠ Bypass naming check: creato {temp_weights}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #risultati migliori con track_buffer basso\n",
        "    tracker = BoTSORT(\n",
        "      frame_rate=25,\n",
        "      device=device,\n",
        "      half=False,\n",
        "      reid_weights=temp_weights,\n",
        "      track_high_thresh=0.25,\n",
        "      track_low_thresh=0.1,\n",
        "      new_track_thresh=0.25,\n",
        "      match_thresh=0.5,\n",
        "      track_buffer=60,\n",
        "      mot20=False,\n",
        "      cmc_method=\"sof\",\n",
        "      name='botsort',\n",
        "      ablation=False,\n",
        "      with_reid=True,\n",
        "      proximity_thresh=0.3,\n",
        "      appearance_thresh=0.25\n",
        "  )\n",
        "\n",
        "\n",
        "\n",
        "    def get_features_bridge(xyxy, img):\n",
        "        crops = []\n",
        "        h, w, _ = img.shape\n",
        "        for box in xyxy:\n",
        "            x1, y1, x2, y2 = map(int, box)\n",
        "            x1, y1, x2, y2 = max(0, x1), max(0, y1), min(w, x2), min(h, y2)\n",
        "            crop = img[y1:y2, x1:x2] if x2 > x1 and y2 > y1 else np.zeros((256, 128, 3), dtype=np.uint8)\n",
        "            crops.append(crop)\n",
        "        return reid_extractor.extract_features(crops)\n",
        "\n",
        "    reid_extractor.get_features = get_features_bridge\n",
        "\n",
        "\n",
        "    # Pulizia: cancelliamo il file temporaneo\n",
        "    if os.path.exists(temp_weights):\n",
        "        os.remove(temp_weights)\n",
        "\n",
        "    # --- 3. HOT-SWAP DEL MODELLO (La parte cruciale) ---\n",
        "    # Ora che il tracker √® inizializzato, buttiamo via il modello che ha caricato lui\n",
        "    # e ci mettiamo dentro il TUO 'reid_extractor' configurato con torchreid.\n",
        "    tracker.model = reid_extractor\n",
        "\n",
        "    # Adattatore: BoTSORT a volte chiama funzioni diverse, le mappiamo tutte al tuo extractor\n",
        "    tracker.model.get_features = lambda x: reid_extractor.extract_features(x)\n",
        "    tracker.model.forward = lambda x: reid_extractor.extract_features(x)\n",
        "\n",
        "    # Colleghiamo il bridge al tracker\n",
        "    tracker.model.get_features = get_features_bridge\n",
        "    # Colleghiamo anche forward per sicurezza (anche se BoTSORT usa get_features)\n",
        "    tracker.model.forward = get_features_bridge\n",
        "\n",
        "\n",
        "\n",
        "    print(\"üîß ReID Custom integrato nel Tracker (Modello standard rimosso).\")\n",
        "\n",
        "    # [BEHAVIOR ADDON 1] CARICAMENTO ROI\n",
        "    # Carichiamo le ROI prima di iniziare il loop sui frame\n",
        "    roi_path = os.path.join(DATASET_ROOT, video_seq, 'roi.json')\n",
        "    rois = {}\n",
        "    roi_names = []\n",
        "    if os.path.exists(roi_path):\n",
        "        with open(roi_path) as f:\n",
        "            rois = json.load(f)\n",
        "        roi_names = sorted(rois.keys()) # Ordine alfabetico essenziale per region_id 1 e 2\n",
        "        print(f\"üìê ROI caricate per behavior: {roi_names}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Nessun file roi.json trovato! Il file behavior sar√† vuoto.\")\n",
        "\n",
        "    # Struttura per accumulare i conteggi: {frame_id: {region_id: count}}\n",
        "    behavior_accumulated = {}\n",
        "\n",
        "    # 4. Preparazione Loop\n",
        "    img_dir = os.path.join(DATASET_ROOT, video_seq, 'img1')\n",
        "    if not os.path.exists(img_dir): raise FileNotFoundError(f\"Manca cartella: {img_dir}\")\n",
        "\n",
        "    frames = sorted(glob.glob(os.path.join(img_dir, \"*.jpg\")) + glob.glob(os.path.join(img_dir, \"*.png\")))\n",
        "    os.makedirs(OUTPUT_RESULTS_DIR, exist_ok=True)\n",
        "    results_txt = []\n",
        "\n",
        "    # --- NUOVO: CONFIGURAZIONE CARTELLE DEBUG ---\n",
        "    debug_base_dir = os.path.join(OUTPUT_RESULTS_DIR, 'debug_frames_smart', video_seq)\n",
        "    debug_raw_dir = os.path.join(debug_base_dir, 'raw')\n",
        "    debug_clean_dir = os.path.join(debug_base_dir, 'clean')\n",
        "\n",
        "    # Pulizia preventiva cartelle debug per evitare file vecchi\n",
        "    if os.path.exists(debug_base_dir):\n",
        "        shutil.rmtree(debug_base_dir)\n",
        "    os.makedirs(debug_raw_dir, exist_ok=True)\n",
        "    os.makedirs(debug_clean_dir, exist_ok=True)\n",
        "\n",
        "    rejection_count = 0\n",
        "\n",
        "    print(f\"\\nüöÄ START TRACKING: {video_seq} ({len(frames)} frames)\")\n",
        "\n",
        "    # 5. LOOP PRINCIPALE (Detection -> Field Filter -> Batch Shadow Filter -> Track)\n",
        "    for i, img_path in enumerate(tqdm(frames)):\n",
        "        frame = cv2.imread(img_path)\n",
        "        if frame is None: continue\n",
        "        fid = i + 1\n",
        "\n",
        "        debug_filename = f\"{fid:06d}.txt\"\n",
        "\n",
        "\n",
        "        # A. Pre-elaborazione\n",
        "        mask = get_field_mask_ransac(frame) # Usa la NUOVA funzione con Ransac\n",
        "        yolo_out = yolo_model.predict(frame, conf=0.25, iou=0.6, verbose=False, imgsz=1088, augment=True, half=True)[0]\n",
        "\n",
        "        # Strutture dati per questo frame\n",
        "        raw_lines = []         # Per debug: tutte le detection YOLO\n",
        "        clean_lines = []       # Per debug: solo quelle finali\n",
        "\n",
        "        # Lista temporanea per i candidati che passano il primo step (Campo)\n",
        "        # Salviamo un dizionario per non perdere conf, cls e coordinate originali\n",
        "        candidates_on_field = []\n",
        "\n",
        "        # --- STEP 1: Estrazione YOLO e Filtro CAMPO ---\n",
        "        for box in yolo_out.boxes:\n",
        "            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
        "            conf = float(box.conf[0].cpu().numpy())\n",
        "            cls = int(box.cls[0].cpu().numpy())\n",
        "\n",
        "            # Calcolo xywh (Interi per le funzioni di filtro)\n",
        "            w, h = int(x2 - x1), int(y2 - y1)\n",
        "            x, y = int(x1), int(y1)\n",
        "            bbox_xywh = [x, y, w, h]\n",
        "\n",
        "            # Salviamo la stringa RAW per debug\n",
        "            line_str = f\"{x1:.2f},{y1:.2f},{x2:.2f},{y2:.2f},{conf:.4f},{cls}\"\n",
        "            raw_lines.append(line_str)\n",
        "\n",
        "            # 1. Filtro Piedi in Campo\n",
        "            if is_feet_in_field(bbox_xywh, mask):\n",
        "                candidates_on_field.append({\n",
        "                    'xywh': bbox_xywh,                 # [x, y, w, h] per i filtri\n",
        "                    'tracker_data': [x1, y1, x2, y2, conf, cls], # [x1, y1, x2, y2, conf, cls] per BoTSORT\n",
        "                    'line_str': line_str               # Per debug CLEAN\n",
        "                })\n",
        "\n",
        "        # --- STEP 2: Filtro OMBRE BATCH (Processa il gruppo) ---\n",
        "        # Lista temporanea per chi sopravvive alle ombre\n",
        "        survivors_shadow = []\n",
        "\n",
        "        if candidates_on_field:\n",
        "            # Estraiamo solo la lista [x,y,w,h] da passare alla funzione batch\n",
        "            input_boxes = [c['xywh'] for c in candidates_on_field]\n",
        "\n",
        "            # La funzione restituisce la lista dei box [x,y,w,h] SOPRAVVISSUTI\n",
        "            final_valid_boxes_shadow = batch_shadow_filtering(input_boxes, frame)\n",
        "\n",
        "            # Recuperiamo i dizionari completi dei sopravvissuti alle ombre\n",
        "            for cand in candidates_on_field:\n",
        "                if cand['xywh'] in final_valid_boxes_shadow:\n",
        "                    survivors_shadow.append(cand)\n",
        "\n",
        "        # --- [NUOVO] STEP 2.5: Filtro BLUR ARTIFACTS (Anti-Ghosting) ---\n",
        "        clean_dets = []\n",
        "\n",
        "        if survivors_shadow:\n",
        "            # Prepariamo gli input specifici per la funzione:\n",
        "            # 1. Lista box [x, y, w, h]\n",
        "            blur_in_boxes = [s['xywh'] for s in survivors_shadow]\n",
        "            # 2. Lista confidenze (l'indice 4 di tracker_data √® la confidenza)\n",
        "            blur_in_confs = [s['tracker_data'][4] for s in survivors_shadow]\n",
        "\n",
        "            # Chiamata alla funzione (con tolleranza verticale stretta = 2px)\n",
        "            blur_out_boxes, _ = clean_blur_artifacts(\n",
        "                blur_in_boxes,\n",
        "                blur_in_confs,\n",
        "                iou_thresh=0.4,\n",
        "                vertical_tol=3,       # <--- Fondamentale per il blur orizzontale\n",
        "                conf_target_thresh=0.40\n",
        "            )\n",
        "\n",
        "            # Ricostruzione finale per il Tracker\n",
        "            # Manteniamo solo i candidati il cui box √® presente nell'output del filtro blur\n",
        "            for s in survivors_shadow:\n",
        "                if s['xywh'] in blur_out_boxes:\n",
        "                    clean_dets.append(s['tracker_data'])\n",
        "                    clean_lines.append(s['line_str'])\n",
        "\n",
        "        # Gestione Debug (Salviamo se c'√® differenza tra Raw e Clean)\n",
        "        has_rejections = len(raw_lines) != len(clean_lines)\n",
        "        if has_rejections:\n",
        "            rejection_count += 1\n",
        "            with open(os.path.join(debug_raw_dir, debug_filename), 'w') as f:\n",
        "                f.write('\\n'.join(raw_lines))\n",
        "            with open(os.path.join(debug_clean_dir, debug_filename), 'w') as f:\n",
        "                f.write('\\n'.join(clean_lines))\n",
        "\n",
        "        # --- STEP 3: Tracking Update ---\n",
        "        dets_array = np.array(clean_dets) if clean_dets else np.empty((0, 6))\n",
        "\n",
        "        # 1. Otteniamo le tracce  dal Tracker BoTSORT\n",
        "        online_targets = tracker.update(dets_array, frame)\n",
        "\n",
        "        # [BEHAVIOR ADDON 2] INIZIALIZZA CONTEGGIO FRAME\n",
        "        # Reset contatori per questo frame\n",
        "        current_frame_counts = {1: 0, 2: 0} # Supporta fino a 2 ROI come da specifiche\n",
        "\n",
        "        # C. Formattazione Output\n",
        "        for t in online_targets:\n",
        "            x1, y1, x2, y2 = t[0], t[1], t[2], t[3]\n",
        "            tid = int(t[4])\n",
        "            conf = t[5]\n",
        "\n",
        "            w_box = x2 - x1\n",
        "            h_box = y2 - y1\n",
        "\n",
        "            # --- LOGICA BEHAVIOR (Center of Basis) ---\n",
        "            # Calcolo il punto centrale dei PIEDI (non del rettangolo intero)\n",
        "            foot_cx = x1 + (w_box / 2)\n",
        "            foot_cy = y2\n",
        "\n",
        "            # Controllo ROI\n",
        "            img_h, img_w = frame.shape[:2] # Dovrebbe essere 1080, 1920\n",
        "\n",
        "            for r_idx, r_name in enumerate(roi_names):\n",
        "                r = rois[r_name]\n",
        "                # Convertiamo ROI normalizzata in Pixel\n",
        "                rx1 = r[\"x\"] * img_w\n",
        "                ry1 = r[\"y\"] * img_h\n",
        "                rx2 = (r[\"x\"] + r[\"width\"]) * img_w\n",
        "                ry2 = (r[\"y\"] + r[\"height\"]) * img_h\n",
        "\n",
        "                # Check inclusione\n",
        "                if rx1 <= foot_cx <= rx2 and ry1 <= foot_cy <= ry2:\n",
        "                    region_id = r_idx + 1 # 1-based index\n",
        "                    if region_id in current_frame_counts:\n",
        "                        current_frame_counts[region_id] += 1\n",
        "                    break # IMPORTANTE: Se √® nella ROI 1, non contarlo nella ROI 2\n",
        "\n",
        "            line = f\"{fid},{tid},{x1:.2f},{y1:.2f},{w_box:.2f},{h_box:.2f},{conf:.2f},-1,-1,-1\"\n",
        "            results_txt.append(line)\n",
        "\n",
        "        # Salviamo i conteggi di questo frame nel dizionario globale\n",
        "        behavior_accumulated[fid] = current_frame_counts\n",
        "\n",
        "    # --- SALVATAGGIO FILE ---\n",
        "\n",
        "    # 1. FILE TRACKING: tracking_VID_GROUP.txt\n",
        "    tracking_filename = f\"tracking_{video_seq}_{group_id}.txt\"\n",
        "    out_file = os.path.join(OUTPUT_RESULTS_DIR, tracking_filename)\n",
        "\n",
        "    with open(out_file, 'w') as f:\n",
        "        f.write('\\n'.join(results_txt))\n",
        "    print(f\"üíæ Risultati Tracking salvati in: {out_file}\")\n",
        "\n",
        "    # 2. FILE BEHAVIOR: behavior_VID_GROUP.txt\n",
        "    behavior_filename = f\"behavior_{video_seq}_{group_id}.txt\"\n",
        "    beh_out_file = os.path.join(OUTPUT_RESULTS_DIR, behavior_filename)\n",
        "\n",
        "    with open(beh_out_file, 'w') as f:\n",
        "        for f_id in sorted(behavior_accumulated.keys()):\n",
        "            counts = behavior_accumulated[f_id]\n",
        "            f.write(f\"{f_id},1,{counts[1]}\\n\")\n",
        "            if len(roi_names) > 1:\n",
        "                f.write(f\"{f_id},2,{counts[2]}\\n\")\n",
        "\n",
        "    print(f\"üìä Risultati Behavior salvati in: {beh_out_file}\")\n",
        "    print(f\"üóëÔ∏è Frame con scarti salvati per debug: {rejection_count}\")\n",
        "\n",
        "    return out_file, beh_out_file\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if os.path.exists(DATASET_ROOT):\n",
        "        try:\n",
        "            # Passiamo sia la sequenza video che il gruppo\n",
        "            track_file, beh_file = run_tracking_pipeline(VIDEO_SEQ, GROUP_ID)\n",
        "            print(f\"\\n‚úÖ Pipeline completata per Video {VIDEO_SEQ} - Gruppo {GROUP_ID}\")\n",
        "        except Exception as e:\n",
        "            print(f\"\\n‚ùå Errore durante l'esecuzione: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "    else:\n",
        "        print(f\"‚ùå Errore Critico: La cartella dataset non esiste: {DATASET_ROOT}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MICp6NOEeAev"
      },
      "source": [
        "# creazione behavior_gt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_y7Xkykgf7Z_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "# --- CONFIGURAZIONE ---\n",
        "base_path = './dataset/test_set_videos'\n",
        "image_width = 1920\n",
        "image_height = 1080\n",
        "\n",
        "# Definizione delle ROI (normalizzate)\n",
        "roi_content = {\n",
        "    1: {\"x\": 0.01, \"y\": 0.01, \"width\": 0.4, \"height\": 0.75}, # ROI 1\n",
        "    2: {\"x\": 0.5, \"y\": 0.35, \"width\": 0.5, \"height\": 0.5}    # ROI 2\n",
        "}\n",
        "\n",
        "def is_point_in_roi(px, py, roi_def, img_w, img_h):\n",
        "    \"\"\"\n",
        "    Verifica se il punto (px, py) cade nella ROI specificata.\n",
        "    Le coordinate della ROI sono normalizzate (0-1), quelle del punto sono in pixel.\n",
        "    \"\"\"\n",
        "    # Conversione ROI da normalizzato a pixel assoluti\n",
        "    rx = roi_def[\"x\"] * img_w\n",
        "    ry = roi_def[\"y\"] * img_h\n",
        "    rw = roi_def[\"width\"] * img_w\n",
        "    rh = roi_def[\"height\"] * img_h\n",
        "\n",
        "    # Verifica inclusione\n",
        "    return (rx <= px <= rx + rw) and (ry <= py <= ry + rh)\n",
        "\n",
        "def generate_behavior_gt():\n",
        "    # Cerca tutte le cartelle video. Poich√© sono numeri (001, 002...),\n",
        "    # filtriamo per assicurarci di prendere solo le directory numeriche.\n",
        "    all_items = sorted(os.listdir(base_path))\n",
        "    video_folders = [os.path.join(base_path, item) for item in all_items if item.isdigit() and os.path.isdir(os.path.join(base_path, item))]\n",
        "\n",
        "    print(f\"Trovate {len(video_folders)} sequenze video in {base_path} (da {os.path.basename(video_folders[0])} a {os.path.basename(video_folders[-1])})\")\n",
        "\n",
        "    for video_folder in video_folders:\n",
        "        video_name = os.path.basename(video_folder)\n",
        "\n",
        "        # DEFINIZIONE PERCORSI AGGIORNATA\n",
        "        gt_track_path = os.path.join(video_folder, 'gt', 'gt.txt')\n",
        "        output_path = os.path.join(video_folder, 'gt', 'behavior_gt.txt') # Ora √® dentro la cartella gt\n",
        "\n",
        "        if not os.path.exists(gt_track_path):\n",
        "            print(f\"ATTENZIONE: gt.txt non trovato per {video_name}, salto.\")\n",
        "            continue\n",
        "\n",
        "        # Dizionario per accumulare i conteggi: frame_id -> {roi_id: count}\n",
        "        frame_counts = {}\n",
        "\n",
        "        # 1. Lettura e Processing del Tracking GT\n",
        "        with open(gt_track_path, 'r') as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split(',')\n",
        "                # Parsing: frame, id, left, top, width, height, ...\n",
        "                try:\n",
        "                    frame_id = int(parts[0])\n",
        "                    # Coordinate Bbox\n",
        "                    left = float(parts[2])\n",
        "                    top = float(parts[3])\n",
        "                    width = float(parts[4])\n",
        "                    height = float(parts[5])\n",
        "                except ValueError:\n",
        "                    continue # Salta righe malformate se ce ne fossero\n",
        "\n",
        "                # Calcolo del punto \"piedi\" (centro del lato inferiore)\n",
        "                foot_x = left + (width / 2.0)\n",
        "                foot_y = top + height\n",
        "\n",
        "                # Inizializza il frame nel dizionario se non esiste\n",
        "                if frame_id not in frame_counts:\n",
        "                    frame_counts[frame_id] = {1: 0, 2: 0}\n",
        "\n",
        "                # Verifica ROI 1\n",
        "                if is_point_in_roi(foot_x, foot_y, roi_content[1], image_width, image_height):\n",
        "                    frame_counts[frame_id][1] += 1\n",
        "\n",
        "                # Verifica ROI 2\n",
        "                if is_point_in_roi(foot_x, foot_y, roi_content[2], image_width, image_height):\n",
        "                    frame_counts[frame_id][2] += 1\n",
        "\n",
        "        # 2. Scrittura del file behavior_gt.txt\n",
        "        sorted_frames = sorted(frame_counts.keys())\n",
        "\n",
        "        if len(sorted_frames) == 0:\n",
        "            print(f\"Nessun dato valido trovato in {video_name}.\")\n",
        "            continue\n",
        "\n",
        "        print(f\"Scrittura behavior_gt.txt per {video_name} in {output_path} ({len(sorted_frames)} frame)...\")\n",
        "\n",
        "        with open(output_path, 'w') as out_f:\n",
        "            for fid in sorted_frames:\n",
        "                # Scrittura ROI 1: frame, region_id, count\n",
        "                out_f.write(f\"{fid},1,{frame_counts[fid][1]}\\n\")\n",
        "                # Scrittura ROI 2: frame, region_id, count\n",
        "                out_f.write(f\"{fid},2,{frame_counts[fid][2]}\\n\")\n",
        "\n",
        "    print(\"\\nGenerazione completata per tutti i video.\")\n",
        "\n",
        "# Esegui la funzione\n",
        "generate_behavior_gt()"
      ]
    }
  ]
}